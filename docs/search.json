[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Analysis for Latent Crohn’s Disease Subgroups Are Identified by Faecal Calprotectin Profiles",
    "section": "",
    "text": "This website presents the analytical reports developed for Latent Crohn’s Disease Subgroups Are Identified by Faecal Calprotectin Profiles by Constantine-Cooke et al. In this work, we use latent class mixed models to identify latent subgroups within the Crohn’s disease patient population based upon the profiles of a biomarker, faecal calprotectin.\nThe analysis pipeline for this project consists of three stages:\n\n\nData cleaning where data obtained from clinicians are reformatted and any data quality issues are dealt with.\n\nModel selection where latent class mixed models are fitted with differing numbers of assumed classes and the most appropriate model is selected.\n\nAssociation testing where potentially significant associations between latent class membership for the optimal model and either data available at diagnosis, treatments, or outcomes are tested for."
  },
  {
    "objectID": "index.html#using-this-website",
    "href": "index.html#using-this-website",
    "title": "Analysis for Latent Crohn’s Disease Subgroups Are Identified by Faecal Calprotectin Profiles",
    "section": "Using this website",
    "text": "Using this website\nThe navigation menu at the top of the page will allow you to navigate through the steps of the analysis pipeline. The code button at the top of each page can be used to show all code blocks instead of clicking on the code buttons for each individual block of code. Moving the mouse pointer over any citations in a report will produce a pop up box with reference details. Clicking on the citation will link to the bibliography at the bottom of the page."
  },
  {
    "objectID": "index.html#software-versions",
    "href": "index.html#software-versions",
    "title": "Analysis for Latent Crohn’s Disease Subgroups Are Identified by Faecal Calprotectin Profiles",
    "section": "Software versions",
    "text": "Software versions\nAll analyses have been generated using R version 4.2.0. See the Session Information sections at the bottom of each report to see the R packages used for that particular analysis and the respective package versions."
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "Analysis for Latent Crohn’s Disease Subgroups Are Identified by Faecal Calprotectin Profiles",
    "section": "",
    "text": "This website has been generated using the Quarto scientific publishing system built on pandoc."
  },
  {
    "objectID": "data-cleaning.html",
    "href": "data-cleaning.html",
    "title": "Data Cleaning",
    "section": "",
    "text": "Codelibrary(tidyverse)\nlibrary(ggdist)\nlibrary(datefixR)\n\nif(!require(pander)) {\n  install.packages(\"pander\")\n}\n\nif(!require(viridis)) {\n  install.packages(\"viridis\")\n}\n\noptions(knitr.table.format = \"pipe\")\nFCcumulative <- read.csv(paste0(\"/Volumes/igmm/cvallejo-predicct/data/\",\n                                \"crohns-inception/20211201/FCcumulative.csv\"))\n\n\nThis analysis, which aims to find latent Crohn’s disease (CD) subgroups with similar faecal calprotectin (FCAL) profiles, uses data from the Crohn’s Disease Inception Cohort (Plevris et al. 2021). The Crohn’s disease inception cohort is a manually validated longitudinal cohort of 375 incident Crohn’s disease (CD) cases. These are all incident CD cases diagnosed between 2005 and 2017 with a diagnosis FCAL of \\geq 250 \\mu g/g, at least one FCAL measurement within the first 12 months of diagnosis, and at least 12 months of follow-up. Figure 1 shows how this cohort was derived.\n\n\nFigure 1: Flowchart of how the Crohn’s disease inception cohort was derived (Plevris et al. 2021) (licensed under CC BY).\n\n\nPreviously, data from this cohort has been used by Plevris et al. (2021) to demonstrate an association between normalising (< 250 \\mu g/g) FCAL within one year of diagnosis and CD disease outcomes. The outcomes considered were hospitalisation, surgery, disease behaviour progression, and a composite end-point of all three outcomes.\nAdditional follow-up data for this cohort has been collated by a medical student into a large multi-sheet Excel file. Plevris had begun formatting the relevant data into a relevant format for modelling of longitudinal and survival outcomes, but this process had not been completed before the data was transferred. This document details the data processing steps undertaken to facilitate modelling.\nThe data is currently in long format: one row per subject with one date column and one associated FCAL value per sample (see Table 1). As such, there are 375 rows in this dataset. This dataset has 147 columns in the dataset. There are up to 45 FCAL measurements with associated date columns, followed by censored status for each of the four outcomes and the associated dates. Some columns are entirely NA.\n\nCodeknitr::kable(FCcumulative[1:5, 1:5], align = \"c\")\n\n\n\nTable 1: First 5 rows and first 5 columns of the dataset.\n\nX\nPatient.number\nFC1..DIAGNOSIS.\nFC1.DATE\nFC2\n\n\n\nOVERALL\n1\n880\n01/01/2015\n770\n\n\nOVERALL\n2\n1080\n24/03/2017\n142\n\n\nOVERALL\n3\n410\n01/01/2009\n1215\n\n\nOVERALL\n4\n400\n14/06/2011\n200\n\n\nOVERALL\n5\n1650\n17/11/2008\n1920"
  },
  {
    "objectID": "data-cleaning.html#data-wrangling",
    "href": "data-cleaning.html#data-wrangling",
    "title": "Data Cleaning",
    "section": "Data wrangling",
    "text": "Data wrangling\nDropping columns\nThe first column lists “Overall” for all subjects. This is believed to be a holdover from the original format of the data, where each outcome for a subject had separate rows. As such, this column can be freely dropped. Additionally, all NA-only columns can also be dropped.\nFurthermore, there are some unnamed columns which give the number of days between the diagnosis FCAL and the date of the FCAL measurement. These columns can also be dropped as they will be recalculated alongside all of the FCAL measurements which do not already have this statistic calculated.\n\nCodeFCcumulative <- FCcumulative[, -1] %>% # drop first column\n  select(function(x) !all(is.na(x))) %>% # drop all NA columns\n  select(!starts_with(\"X\")) # Drop columns giving days from diagnosis\n\n\nDropping these columns results in 102 columns being retained.\nConverting to long format\nFor analysis, it is necessary to convert the data to long format: one row per measurement. fix_dates() from the datefixR R package is used to convert the dates to R’s Date type. We also map censoring status indicators for endpoints to either 0 (censored) or 1 (observed).\nTable 2 presents the first 5 rows of the long format table.\n\nCodeids <- c()\nvalues <- c()\ndates <- c()\nhosps <- c()\ndate.hosps <- c()\nprogs <- c()\ndate.progs <- c()\nsurgs <- c()\ndate.surgs <- c()\n\ncomposites <- c()\ncomposites.days <- c()\n\n\nfor (subject in 1:nrow(FCcumulative)) {\n  is.measurement <- TRUE\n  measurement <- 0\n  while (is.measurement & measurement < 45) {\n    measurement <- measurement + 1\n    value <- FCcumulative[subject, 2 * measurement]\n    if (is.na(value)) {\n      is.measurement <- FALSE\n    } else {\n    date <- FCcumulative[subject, (2 * measurement) + 1]\n    \n    hosp <- FCcumulative[subject, \"IBD.HOSPITALISATION.POST.1.YEAR..Y.N.\"]\n    if (hosp == \"Y\") hosp <- 1\n    if (hosp == \"N\") hosp <- 0\n    if (hosp == \"N \") hosp <- 0\n    date.hosp <- FCcumulative[subject, \"DATE.OF.FIRST.IBD.HOSPITALISATION.POST.1.YEAR\"]\n    if (date.hosp == \"N\" | date.hosp == \"NO\") {\n      date.hosp <- FCcumulative[subject, \"DATE.OF.LAST.FOLLOW.UP\"]\n    } \n    \n    prog <- FCcumulative[subject, \"DISEASE.PROGRESSION.POST.1.YEAR..Y.N...B1.B2.3..B2.B3.new.perianal.\"]\n    if (prog == \"Y\") prog <- 1\n    if (prog == \"N\") prog <- 0\n    \n    date.prog <- FCcumulative[subject, \"DATE.OF.DISEASE.PROGRESSION.POST.1.YEAR\"]\n    if (date.prog == \"N\" | date.prog == \"NO\") {\n      date.prog <- FCcumulative[subject, \"DATE.OF.LAST.FOLLOW.UP\"]\n    }\n    \n    surg <- FCcumulative[subject, \"RESECTIONAL.SURGERY.POST.1.YEAR..Y.N.\"]\n    if (surg == \"Y\") surg <- 1\n    if (surg == \"y\") surg <- 1\n    if (surg == \"N\") surg <- 0\n    if (surg == \"NO\") surg <- 0\n\n    date.surg <- FCcumulative[subject, \"DATE.OF.FIRST.SURGERY.POST.1.YEAR\"]\n    if (date.surg == \"N\" | date.surg == \"NO\") {\n      date.surg <- FCcumulative[subject, \"DATE.OF.LAST.FOLLOW.UP\"]\n    }\n    \n    composite  <- FCcumulative[subject, \"COMPOSITE.CENSOR\"]\n    composite.day <- FCcumulative[subject, \"COMPOSITE.DAYS\"]\n    \n    ids <- c(ids, subject); values <- c(values, value); dates <- c(dates, date)\n    hosps <- c(hosps, hosp); date.hosps <- c(date.hosps, date.hosp)\n    progs <- c(progs, prog); date.progs <- c(date.progs, date.prog)\n    surgs <- c(surgs, surg); date.surgs <- c(date.surgs, date.surg)\n    composites <- c(composites, composite)\n    composites.days <- c(composites.days, composite.day)\n    }\n  }\n}\n\nFCcumulativeLong <- data.frame(id = ids,\n                               date = dates, \n                               value = values,\n                               hosp = hosps,\n                               hosp.date = date.hosps,\n                               prog = progs,\n                               prog.date = date.progs,\n                               surg = surgs,\n                               surg.date = date.surgs,\n                               composite = composites,\n                               composites.day = composites.days)\n\nFCcumulativeLong <- fix_dates(FCcumulativeLong,\n                              c(\"date\",\n                                \"hosp.date\",\n                                \"prog.date\",\n                                \"surg.date\"))\n\nFCcounts <- as.vector(table(FCcumulativeLong$id))\n\nmax.followup <- max(c(FCcumulativeLong$hosp.date,\n                      FCcumulativeLong$prog.date,\n                      FCcumulativeLong$surg.date))\n\nknitr::kable(FCcumulativeLong[1:5, 1:3], align = \"c\")\n\n\n\nTable 2: First 5 observations of the long-format table.\n\nid\ndate\nvalue\n\n\n\n1\n2015-01-01\n880\n\n\n1\n2015-06-09\n770\n\n\n1\n2016-10-28\n90\n\n\n2\n2017-03-24\n1080\n\n\n2\n2017-07-04\n142\n\n\n\n\n\n\nConverting to long format reveals there are 3616 FCAL samples reported in this dataset with mean 9.64 measurements per subject and median 7 (Q1: 5, Q3: 12) measurements per subject. From these quantiles, it appears the distribution of number of FCAL measurements is highly skewed. Figure 2 shows this distribution in greater detail and demonstrates the substantial skew observed.\n\nCodeFCcountsdf <- tibble(counts = FCcounts)\n\nFCcountsdf %>%\n  ggplot(aes(x = counts)) +\n  stat_slab(size = 0.8, alpha = 0.9, fill = \"#235789\", color = \"#235789\") +\n  geom_dots(fill = \"#ed1c24\",\n            color = \"#ed1c24\",\n            alpha = 0.5,\n            binwidth = 1,\n            dotsize = 0.5) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank()) +\n  xlab(\"Number of FCAL observations per subject\") +\n  ylab(\"\") +\n  ggtitle(\"Distribution of Number of FCAL Measurements per Subject\",\n          \"Crohn's disease inception cohort\")\n\n\n\nFigure 2: Distribution of number of FCAL measurements per subject."
  },
  {
    "objectID": "data-cleaning.html#quality-control",
    "href": "data-cleaning.html#quality-control",
    "title": "Data Cleaning",
    "section": "Quality control",
    "text": "Quality control\n\nCodelargeFC <- subset(FCcumulativeLong, value > 2500)\n\n\nFrom Figure 3, we can see there are FCAL measurements greater than 2500 \\mu g/g despite the assay used by NHS Lothian having an upper accuracy limit of 2500 with values greater than this normally reported as “> 2500”. There are 10 measurements which are above this threshold.\n\nCodeFCcumulativeLong %>%\n  ggplot(aes(x = value, y = NULL)) +\n  stat_slab(size = 0.8, alpha = 0.5, fill = \"#235789\") +\n  geom_dots(aes(color = value > 2500, fill = value > 2500),\n            binwidth = 10,\n            size = 1,\n            side = \"bottom\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank()) +\n  xlab(\"FCAL (µg/g)\") +\n  ylab(\"\") +\n  ggtitle(\"Distribution of FCAL Measurements\",\n          \"Crohn's disease inception cohort (before quality control)\") +\n  scale_fill_manual(values = c(\"#235789\", \"#ed1c24\")) + \n  scale_color_manual(values = c(\"#235789\", \"#ed1c24\")) +\n  labs(fill = \"FCAL > 2500\", color = \"FCAL > 2500\")\n\n\n\nFigure 3: Distribution of FCAL values. Red dots indicate FCAL values above 2500.\n\n\n\n\nFCAL > 2500 has been mapped to FCAL = 2500 which results in the distribution seen in Figure 4.\n\nCodeFCcumulativeLong$value <- ifelse(FCcumulativeLong$value > 2500,\n                           2500,\n                           FCcumulativeLong$value)\n\nFCcumulativeLong %>%\n  ggplot(aes(x = value, y = NULL)) +\n  stat_slab(size = 0.8, alpha = 0.5, fill = \"#235789\") +\n  geom_dots(binwidth = 10, size = 1, side = \"bottom\", color = \"#235789\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank()) +\n  xlab(\"FCAL (µg/g)\") +\n  ylab(\"\") +\n  ggtitle(\"Distribution of FCAL Measurements (After Quality Control)\",\n          \"Crohn's disease inception cohort\")\n\n\n\nFigure 4: Distribution of FCAL values once values above 2500 are mapped to 2500."
  },
  {
    "objectID": "data-cleaning.html#time-retiming",
    "href": "data-cleaning.html#time-retiming",
    "title": "Data Cleaning",
    "section": "Time retiming",
    "text": "Time retiming\nWe will create a new variable, time, which will give the number of days since the subject’s diagnostic FCAL for each FCAL measurement. It follows time = 0 for each subject’s diagnostic measurement. time has been scaled to be on a year-scale. Table 3 presents the first 10 FCAL observations.\n\nCodetempdf <- data.frame()\n\nfor (subject in seq_along(FCcumulative$Patient.number)) {\n  temp <- subset(FCcumulativeLong, id == subject)\n  # Should already be in order, but worth ensuring\n  temp <- temp[order(temp$date), ]\n  temp$time <- as.numeric(temp$date - temp$date[1]) / 365.25\n  temp$hosp.date <- as.numeric(temp$hosp.date - temp$date[1]) / 365.25\n  temp$prog.date <- as.numeric(temp$prog.date - temp$date[1]) / 365.25\n  temp$surg.date <- as.numeric(temp$surg.date - temp$date[1]) / 365.25\n  tempdf <- rbind(tempdf, temp)\n}\n\nFCcumulativeLong <- tempdf\nrm(tempdf)\n\nknitr::kable(FCcumulativeLong[1:10, c(1, 2, 3, 12)], row.names = FALSE)\n\n\n\nTable 3: FCAL measurements with time since diagnosis.\n\nid\ndate\nvalue\ntime\n\n\n\n1\n2015-01-01\n880\n0.0000000\n\n\n1\n2015-06-09\n770\n0.4353183\n\n\n1\n2016-10-28\n90\n1.8234086\n\n\n2\n2017-03-24\n1080\n0.0000000\n\n\n2\n2017-07-04\n142\n0.2792608\n\n\n2\n2018-03-22\n363\n0.9938398\n\n\n2\n2018-06-26\n376\n1.2566735\n\n\n2\n2018-10-02\n1210\n1.5249829\n\n\n3\n2009-01-01\n410\n0.0000000\n\n\n3\n2009-05-12\n1215\n0.3586585\n\n\n\n\n\n\nFrom these data, we are able to plot subject-specific FCAL trajectories. As expected, a high degree of heterogeneity is observed (Figure 5).\n\nCodeFCcumulativeLong %>% \n  ggplot(aes(x = time, y = log(value), color = factor(id))) +\n  geom_line(alpha = 0.2) +\n  geom_point(alpha = 0.6) +\n  theme_minimal() + \n  scale_color_manual(values = viridis::viridis(375)) +\n  guides(color = \"none\") +\n  xlab(\"Time (years)\") +\n  ylab(\"Log FCAL\") +\n  ggtitle(\"Spaghetti Plot of FCAL Trajectories\",\n          \"Subject-specific trajectories show a high degree of heterogeneity\")\n\n\n\nFigure 5: Spaghetti plot of all FCAL trajectories in the Crohn’s disease inception cohort."
  },
  {
    "objectID": "data-cleaning.html#time-cut-off",
    "href": "data-cleaning.html#time-cut-off",
    "title": "Data Cleaning",
    "section": "Time cut-off",
    "text": "Time cut-off\nAs a sparse number of measurements at the end of the follow-up may result in a longitudinal model performing poorly for this time-period, a cut-off for time should be mandated. As such, we have performed an exploratory analysis to inform our decision on the value for this cut-off (Table 4). At least three measurements in the eligible time period have been mandated for a subject to be included.\n\nCodeyears <- seq(2, 10)\nmean.n <- c()\nmedian.n <- c()\nIQR.n <- c()\n\nfor (year in years) {\n  # restrict to measurements within threshold\n  temp <- subset(FCcumulativeLong, time <= year) \n  # restrict to subjects with three measurements within threshold\n  temp <- subset(temp, id %in% (unique(temp$id))[table(temp$id) > 3])\n  counts <- table(temp$id)\n  mean.n <- c(mean.n, round(mean(counts), 2))\n  median.n <- c(median.n, median(counts))\n  IQR.n <- c(IQR.n,  IQR(counts))\n}\n\nknitr::kable(data.frame(years = years,\n                     mean.n = mean.n,\n                     median.n = median.n,\n                     IQR.n = IQR.n),\n             col.names = c(\"Year cut-off\", \"Mean\", \"Median\", \"IQR\"))\n\n\n\nTable 4: Mean, median and interquartile range for number of FCAL measurements per subject for different time cut-offs.\n\nYear cut-off\nMean\nMedian\nIQR\n\n\n\n2\n5.72\n5\n2\n\n\n3\n6.83\n6\n3\n\n\n4\n7.79\n7\n5\n\n\n5\n8.64\n7\n5\n\n\n6\n9.30\n8\n5\n\n\n7\n9.82\n8\n6\n\n\n8\n10.16\n8\n7\n\n\n9\n10.36\n8\n7\n\n\n10\n10.50\n8\n7\n\n\n\n\n\n\nFrom this table, measurements within six years of diagnosis seems a sensible cut-off. However, we have ultimately decided five years is a better choice in order to be able to directly compare with other studies, for example the IBSEN study (Henriksen et al. 2006), which describe 5-year disease activity trajectories.\nBased on the data cleaning step of the pipeline, restricting measurements to within 5 years of diagnosis seemed appropriate. Additionally, we will only consider subjects with at least 3 FCAL measurements within this time period. This report reports descriptive statistics for the cohort after this inclusion criteria is applied.\n\nCodeFCcumulativeLong <- subset(FCcumulativeLong, time < 5)\nsubjects.with.3 <- unique(FCcumulativeLong$id)[table(FCcumulativeLong$id) >=3]\nFCcumulativeLong <- subset(FCcumulativeLong, id %in% subjects.with.3)\n\n\nThere are 356 CDI subjects which meet the criteria of having at least 3 FCAL measurements within 5 years of diagnosis.\n\nCodeFCcounts <- as.vector(table(FCcumulativeLong$id))\n\n\nThere are 2856 FCAL samples reported with mean 8.02 measurements per subject and median 7 (Q1: 5, Q3: 10) measurements per subject. The below plots are updated version of previous plots: updated to only include subjects which meet this criteria.\n\nCodeFCcountsdf <- tibble(counts = FCcounts)\n\np <- FCcountsdf %>%\n  ggplot(aes(x = counts)) +\n  stat_slab(size = 0.8, alpha = 0.9, fill = \"#235789\", color = \"#235789\") +\n  geom_dots(fill = \"#ed1c24\",\n            color = \"#ed1c24\",\n            alpha = 0.5,\n            binwidth = 1,\n            dotsize = 0.22) +\n  theme_minimal() +\n  theme(axis.text.y = element_blank()) +\n  xlab(\"Number of FCAL observations per subject\") +\n  ylab(\"\") +\n  ggtitle(\"Distribution of Number of FCAL Measurements per Subject\",\n          paste(\"Crohn's disease inception cohort,\",\n                \"(inclusion: >2 FCAL measurements within 5 years)\"))\nggsave(\"plots/dist1.png\" , p, width = 8, height = 4.5, units = \"in\")\nprint(p)\n\n\n\n\n\nCodep <- FCcountsdf %>%\n  ggplot(aes(x = counts)) +\n  geom_histogram(binwidth = 1, fill = \"#479AA7\", col = \"#327c88\") +\n  theme_minimal() +\n  xlab(\"Faecal calprotectin measurement frequency\") +\n  ylab(\"Subjects\")\nggsave(\"paper/fcal-dist.png\", p, width = 8, height = 4.5, units = \"in\")\nggsave(\"paper/fcal-dist.pdf\", p, width = 8, height = 4.5, units = \"in\")\n\n\n\nCodep <- FCcumulativeLong %>%\n  ggplot(aes(x = value, y = NULL)) +\n  stat_slab(size = 0.8, alpha = 0.5, fill = \"#235789\") +\n  geom_dots(color = \"#235789\",\n            fill = \"#235789\",\n            binwidth = 10,\n            size = 1,\n            side = \"bottom\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank()) +\n  xlab(\"FCAL (µg/g)\") +\n  ylab(\"\") +\n  ggtitle(\"Distribution of FCAL Measurements\",\n          paste(\"Crohn's disease inception cohort,\",\n                \"(inclusion: >2 FCAL measurements within 5 years)\"))\nggsave(\"plots/dist2.png\", p, width = 8, height = 4.5, units = \"in\")\nprint(p)\n\n\n\n\n\nCodeFCcumulativeLong %>% \n  ggplot(aes(x = time, y = log(value), color = factor(id))) + geom_line(alpha = 0.2) +\n  geom_point(alpha = 0.6) +\n  theme_minimal() + \n  scale_color_manual(values = viridis::viridis(375)) +\n  guides(color = \"none\") +\n  xlab(\"Time (years)\") +\n  ylab(\"Log FCAL\") +\n  ggtitle(\"Spaghetti Plot of FCAL Trajectories\",\n          \"Subject-specific trajectories show a high degree of heterogeneity\")\n\n\n\n\nIt should be noted June 2019 is the last month of followup"
  },
  {
    "objectID": "data-cleaning.html#data-saving",
    "href": "data-cleaning.html#data-saving",
    "title": "Data Cleaning",
    "section": "Data saving",
    "text": "Data saving\nThe tidied data, with a 5-year cut-off applied, has been saved as a RDS file for the model selection step of the analysis pipeline.\n\nCodesaveRDS(FCcumulativeLong,\n        paste0(\"/Volumes/igmm/cvallejo-predicct/processed-data/cdi/\",\n               \"FCcumulativeLongInc.RDS\"))"
  },
  {
    "objectID": "data-cleaning.html#session-information",
    "href": "data-cleaning.html#session-information",
    "title": "Data Cleaning",
    "section": "Session information",
    "text": "Session information\n\n\nR version 4.2.0 (2022-04-22)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_GB.UTF-8||en_GB.UTF-8||en_GB.UTF-8||C||en_GB.UTF-8||en_GB.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: viridis(v.0.6.2), viridisLite(v.0.4.0), pander(v.0.6.5), datefixR(v.0.1.4), ggdist(v.3.1.1), forcats(v.0.5.1), stringr(v.1.4.0), dplyr(v.1.0.9), purrr(v.0.3.4), readr(v.2.1.2), tidyr(v.1.2.0), tibble(v.3.1.7), ggplot2(v.3.3.6) and tidyverse(v.1.3.1)\nloaded via a namespace (and not attached): Rcpp(v.1.0.8.3), lubridate(v.1.8.0), assertthat(v.0.2.1), digest(v.0.6.29), utf8(v.1.2.2), R6(v.2.5.1), cellranger(v.1.1.0), backports(v.1.4.1), reprex(v.2.0.1), evaluate(v.0.15), highr(v.0.9), httr(v.1.4.3), pillar(v.1.7.0), rlang(v.1.0.2), readxl(v.1.4.0), rstudioapi(v.0.13), rmarkdown(v.2.14), textshaping(v.0.3.6), labeling(v.0.4.2), htmlwidgets(v.1.5.4), munsell(v.0.5.0), broom(v.0.8.0), compiler(v.4.2.0), modelr(v.0.1.8), xfun(v.0.30), systemfonts(v.1.0.4), pkgconfig(v.2.0.3), htmltools(v.0.5.2), tidyselect(v.1.1.2), gridExtra(v.2.3), fansi(v.1.0.3), crayon(v.1.5.1), tzdb(v.0.3.0), dbplyr(v.2.1.1), withr(v.2.5.0), grid(v.4.2.0), distributional(v.0.3.0), jsonlite(v.1.8.0), gtable(v.0.3.0), lifecycle(v.1.0.1), DBI(v.1.1.2), magrittr(v.2.0.3), scales(v.1.2.0), cli(v.3.3.0), stringi(v.1.7.6), farver(v.2.1.0), renv(v.0.15.4), fs(v.1.5.2), xml2(v.1.3.3), ragg(v.1.2.2), ellipsis(v.0.3.2), generics(v.0.1.2), vctrs(v.0.4.1), tools(v.4.2.0), glue(v.1.6.2), hms(v.1.1.1), fastmap(v.1.1.0), yaml(v.2.3.5), colorspace(v.2.0-3), rvest(v.1.0.2), knitr(v.1.39) and haven(v.2.5.0)"
  },
  {
    "objectID": "data-cleaning.html#acknowledgments",
    "href": "data-cleaning.html#acknowledgments",
    "title": "Data Cleaning",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis work is funded by the Medical Research Council & University of Edinburgh via a Precision Medicine PhD studentship (MR/N013166/1, to NC-C)."
  },
  {
    "objectID": "data-cleaning.html#author-contributions",
    "href": "data-cleaning.html#author-contributions",
    "title": "Data Cleaning",
    "section": "Author contributions",
    "text": "Author contributions\nNC-C performed the data cleaning detailed in this report. NP and INSERT extracted the data from electronic health records and formatted the data in the form initially described."
  },
  {
    "objectID": "data-cleaning.html#reuse",
    "href": "data-cleaning.html#reuse",
    "title": "Data Cleaning",
    "section": "Reuse",
    "text": "Reuse\nLicensed by CC BY  unless otherwise stated."
  },
  {
    "objectID": "selection.html",
    "href": "selection.html",
    "title": "Model Selection",
    "section": "",
    "text": "Codeset.seed(123)\n\n# Fitting the LCMMs takes around three hours (when running single threaded)\n# if cache.models is true then the saved model objects will be used instead of\n# refitting the models\ncache.models <- TRUE\n\n##########################\n#--     Packages       --#\n##########################\n\nlibrary(tidyverse)\n## Modelling ##\nlibrary(lcmm)\nlibrary(splines)\n## Presentation ##\nlibrary(htmltools)\nlibrary(patchwork)\nlibrary(ggdist)\nlibrary(grid)\nlibrary(ggalluvial)\n\nif (!require(DT)) {\n  install.packages(\"DT\")\n}\n\n##########################\n#--     Data read      --#\n##########################\n\nFCcumulative <- readRDS(paste0(\"/Volumes/igmm/cvallejo-predicct/\",\n                               \"processed-data/cdi/\",\n                               \"FCcumulativeLongInc.RDS\")\n                        )\n\n###########################################\n#-- Create directories and readme files --#\n###########################################\n\nfileConn <- file(\"plots/README.md\")\nwriteLines(c(\"# README\",\n             \"\",\n             paste(\"This directory contains plots created by the analysis\", \n                   \"but are not provided as figures (supplementary or\", \n                   \"otherwise) in the paper\")),\n           fileConn)\nclose(fileConn)\n\n\nif (!dir.exists(\"cache\")) dir.create(\"cache\")\nfileConn <- file(\"cache/README.md\")\nwriteLines(c(\"# README\",\n             \"\",\n             paste(\"This directory stores cached versions of R objects which\",\n                   \"take a long time to create (such as LCMM fit objects).\")),\n           fileConn)\nclose(fileConn)\n\nif (!dir.exists(\"paper\")) dir.create(\"paper\")\nfileConn <- file(\"paper/README.md\")\nwriteLines(c(\"# README\",\n             \"\",\n             paste(\"This directory contains all figures used in the paper.\",\n                   \"The sup subdirectory contains supplementary figures\")),\n           fileConn)\nclose(fileConn)\n\nif (!dir.exists(\"paper/sup\")) dir.create(\"paper/sup\")\n\nif (!dir.exists(\"plots/residuals\")) {\n  dir.create(\"plots/residuals\")\n}\n\n########################\n#-- Custom Functions --#\n########################\n\n# Build DT::datatable objects from matrix of fit statistics\nDTbuild <- function(hlme.metric, caption) {\n  hlme.metrics <- cbind(hlme.metrics, group = seq(1, nrow(hlme.metrics)))\n  hlme.metrics <- hlme.metrics[, c(4, 1, 2, 3)]\n  DT::datatable(round(as.data.frame(hlme.metrics), 2),\n                options = list(dom = 't'),\n                caption = tags$caption(\n                  style = 'text-align: center;',\n                  h3(caption)),\n                style =\"bootstrap4\",\n                rownames = FALSE,\n                colnames = c(\"Classes\",\n                             \"Maximum log-likelihood\",\n                             \"AIC\",\n                             \"BIC\"),\n                escape = FALSE)\n}\n\n#' Spaghetti plots of each class\n#' @param models list containing HLME objects\n#' @param G How many classes does the model assume?\n#' @param log Logical. Should plots be on log scale\n#' @param indi Logical. Should separate plots for each class be generated?\n#' @param multi Logical. Should all plots be plotted alongside each other? \n#' @param tmax Maximum observation period\n#' @param column Logical. Should all sub-plots be in a single column? Defaults\n#'   to false (two columns)\n#' @param prob.cutoff Posterior probability cut-off for subjects to be included\n#'   as trajectories\n#' @param mapping Numeric vector which gives reordering of plots in a\n#'   multiplot. Need to take into account plots are generated by column - not\n#'   row\n#' @param sizes Output latent class sizes\n#' @param save Logical. Should sub figure labels be generated?\nspaghetti_plot <- function(FCcumulative,\n                           models,\n                           G,\n                           log = TRUE,\n                           indi = FALSE, \n                           multi = TRUE,\n                           tmax = 5,\n                           column =  FALSE, \n                           pprob.cutoff = NA,\n                           sizes = FALSE,\n                           mapping = NULL,\n                           save = FALSE){\n  \n  if(!is.na(pprob.cutoff)) {\n    pprob.cutoffs <- c()\n    \n    for (subject in unique(models[[G]]$pprob$id)){\n      temp <- subset(models[[G]]$pprob, id == subject)\n      pprob <- temp[, 2 + temp$class]\n      if (pprob > pprob.cutoff) {\n        pprob.cutoffs <- c(pprob.cutoffs, subject)\n      }\n    }\n    FCcumulative <- subset(FCcumulative, id %in% pprob.cutoffs)\n  }\n  \n  if (indi){\n    spaghetti_plot_sub(FCcumulative = FCcumulative, \n                       models = models,\n                       G = G,\n                       log = log,\n                       multi = FALSE,\n                       tmax = tmax,\n                       column = column,\n                       pprob = pprob,\n                       sizes = sizes,\n                       mapping = mapping)\n    }\n  if (multi){\n      spaghetti_plot_sub(FCcumulative = FCcumulative, \n                         models = models,\n                         G = G,\n                         log = log,\n                         multi = TRUE,\n                         tmax = tmax,\n                         column = column,\n                         pprob = pprob,\n                         sizes = sizes,\n                         mapping = mapping,\n                         save = save)\n  }\n}\n\nspaghetti_plot_sub <- function(FCcumulative,\n                           models,\n                           G,\n                           multi = FALSE,\n                           log,\n                           unit,\n                           tmax = 5,\n                           column = column,\n                           pprob = NA,\n                           sizes = sizes,\n                           mapping = mapping,\n                           save = save){\n  \n  labels <- c(\"A\", \"C\", \"B\", \"D\")\n  \n  time <- seq(0, tmax, by = 0.01)\n  \n  if (column) {\n    # use single column layout for sub-plots\n    layout <- matrix(seq(1, G),\n                   ncol = 1,\n                   nrow = G)\n  } else {\n    # use two column layout for sub-plots\n      layout <- matrix(seq(1, 2 * ceiling(G / 2)),\n                   ncol = 2,\n                   nrow = ceiling(G / 2))\n  }\n  # Set up the page\n  pushViewport(viewport(layout = grid.layout(nrow(layout), ncol(layout))))\n  \n  data_pred <- data.frame(time = time)\n  pred <- predictY(models[[G]],\n                   data_pred,\n                   var.time = \"time\",\n                   draws = TRUE)\n  lcmm_uit <- as.data.frame(pred$pred)\n  lcmm_uit$time <- time\n  \n  if(is.null(mapping)) {\n    mapping <- 1:G\n  }\n  \n  \n  for (g in mapping) {\n    matchidx <- as.data.frame(which(layout == g, arr.ind = TRUE))\n    id.group <- models[[G]]$pprob[models[[G]]$pprob[, 2] == mapping[g], 1]\n    if (sizes) {\n      message(\"There are \",\n              length(id.group),\n              \" subjects in class \",\n              g,\n              \".\")\n    }\n    if(!log){\n      p[[g]] <- ggplot(data = subset(FCcumulative, id %in% id.group),\n                     aes(x = time, y = exp(value))) +\n      geom_line(aes(group = id), alpha = 0.1) +\n      theme_minimal() + \n      geom_line(data = lcmm_uit,\n                aes(x = time,\n                    y = exp(lcmm_uit[, paste0(\"Ypred_class\", mapping[g])])),\n                size = 1.5,\n                col = \"red\") +\n      geom_line(data = lcmm_uit,\n                aes(x = time,\n                    y = exp(lcmm_uit[, paste0(\"lower.Ypred_class\",\n                                              mapping[g])])),\n                col = \"red\",\n                lty = 2) +\n      geom_line(data = lcmm_uit,\n                aes(x = time,\n                    y = exp(lcmm_uit[, paste0(\"upper.Ypred_class\",\n                                              mapping[g])])),\n                col = \"red\",\n                lty = 2) +\n      xlab(\"Time (years)\") +\n      ylab(\"FCAL (μg/g)\") +\n      ylim(0, 2500)\n    } else{\n       p[[g]] <- ggplot(data = subset(FCcumulative, id %in% id.group),\n                     aes(x = time, y = value)) +\n      geom_line(aes(group = id), alpha = 0.1) +\n      theme_minimal() + \n      geom_line(data = lcmm_uit,\n                aes(x = time, y = lcmm_uit[, paste0(\"Ypred_class\",\n                                                    mapping[g])]),\n                size = 1.5,\n                col = \"red\") +\n      geom_line(data = lcmm_uit,\n                aes(x = time, y = lcmm_uit[, paste0(\"lower.Ypred_class\",\n                                                    mapping[g])]),\n                col = \"red\",\n                lty = 2) +\n      geom_line(data = lcmm_uit,\n                aes(x = time, y = lcmm_uit[, paste0(\"upper.Ypred_class\",\n                                                    mapping[g])]),\n                col = \"red\",\n                lty = 2) +\n      geom_hline(yintercept = log(250),\n                 color = \"#007add\",\n                 lty = 3,\n                 size = 1.5) +\n      xlab(\"Time (years)\") +\n      ylab(\"Log (FCAL (μg/g))\") +\n      ylim(2, log(2500))\n    }\n    if (multi) {\n      if (save) {\n        # Add subfigure labels\n        print(p[[g]] +\n                ggtitle(labels[g]) +\n                theme(plot.title = element_text(face = \"bold\",\n                                                 size = 20)),\n              vp = viewport(layout.pos.row = matchidx$row,\n                            layout.pos.col = matchidx$col)\n              )\n      } else {\n        print(p[[g]],\n              vp = viewport(layout.pos.row = matchidx$row,\n                            layout.pos.col = matchidx$col)\n              )\n      }\n    } else {\n      print(p[[g]])\n    }\n  }\n}\n\n\nTo achieve our aims of identifying classes within the CD population with similar FCAL profiles, we use latent class mixed models (LCMMs) with natural cubic spline formulations for the fixed and random effects components. LCMMs are an extension of linear mixed effects models with an added fixed effect class-specific component. Class membership in a LCMM is given via a multinomial logistic model.\nPreviously, we have investigated using polynomial regression, models with an I-splines link function, and models which use Gaussian radial basis functions (GRBFs) in order to model the fixed and random components of an LCMM. Unfortunately, the polynomial regression and I-splines link approaches demonstrated inflexibility and, in the case of polynomial regression behaved erratically near the ends of the time period.\nThe GRBF approach was very sensitive to l, a length scale parameter, and the iterative Marquardt algorithm implemented in the {lcmm} package did not converge in many cases- possibly due the number of parameters required to be estimated and/or the Runge phenomenon (Fornberg and Zuev 2007).\nThis has led us to consider an approach using natural cubic splines which has a few notable advantages (Elhakeem et al. 2022):\n\nLess parameters need to be estimated than either a Gaussian radial basis function regression model or a polynomial regression model with the same flexibility . This reduces the time complexity when fitting the model and in the future may also make extensions more practically feasible.\nNatural cubic splines enforce linearity between t_0 and the first knot and between the last knot and t_\\text{max} which ensures the model does not behave erratically in these sometimes problematic areas.\nNatural cubics are not highly sensitive to a continuous parameter and instead requires only K, the number of knots, to be tuned: being robust to where the knots themselves are placed."
  },
  {
    "objectID": "selection.html#formal-defintions",
    "href": "selection.html#formal-defintions",
    "title": "Model Selection",
    "section": "Formal defintions",
    "text": "Formal defintions\nFor formal definitions of the models and statistics we have used in the work, see the supplementary material for our paper."
  },
  {
    "objectID": "selection.html#the-crohns-disease-inception-cohort",
    "href": "selection.html#the-crohns-disease-inception-cohort",
    "title": "Model Selection",
    "section": "The Crohn’s Disease Inception Cohort",
    "text": "The Crohn’s Disease Inception Cohort\nThe background for the data we will fit to the models and an explanation of the data processing steps implemented can be found on a dedicated page. Due to the distribution of the FCAL values (Figure 1), FCAL values have been log-transformed prior to model fitting (Figure 2).\n\nCodeFCcumulative %>%\n  ggplot(aes(x = value, y = NULL)) +\n  stat_slab(size = 0.8, alpha = 0.5, fill = \"#235789\") +\n  geom_dots(binwidth = 10, size = 1, side = \"bottom\", color = \"#235789\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank()) +\n  xlab(\"FCAL (µg/g)\") +\n  ylab(\"\") +\n  ggtitle(\"Distribution of FCAL Measurements\",\n          \"Crohn's disease inception cohort\")\n\n\n\nFigure 1: Distribution of FCAL values when in measurement units.\n\n\n\n\n\nCodeFCcumulative$value <- log(FCcumulative$value)\n\nFCcumulative %>%\n  ggplot(aes(x = value, y = NULL)) +\n  stat_slab(size = 0.8, alpha = 0.5, fill = \"#235789\") +\n  geom_dots(binwidth = 0.02, size = 1, side = \"bottom\", color = \"#235789\") +\n  theme_minimal() +\n  theme(axis.text.y = element_blank()) +\n  xlab(\"Log (FCAL (µg/g))\") +\n  ylab(\"\") +\n  ggtitle(\"Distribution of Log-Transformed FCAL Measurements\",\n          \"Crohn's disease inception cohort\")\n\n\n\nFigure 2: Distribution of FCAL values after a log transformation has been applied."
  },
  {
    "objectID": "selection.html#model-fitting",
    "href": "selection.html#model-fitting",
    "title": "Model Selection",
    "section": "Model fitting",
    "text": "Model fitting\nLCMMs with 2 - 6 assumed classes are considered. As recommended by Proust-Lima, Philipps, and Liquet (2017), a model is initially fitted with 1 class (I.E a regular linear mixed effects model) which is used to sample initial values in a grid search approach which attempts to find optimal models for each assumed number of classes based upon maximum likelihood. The trajectory of this linear mixed effects model is given by Figure 3.\nFor the fixed and random components of each model, we will consider natural cubic splines of time with three knots (I.E five fixed points including the boundaries of the splines. The knots for the natural cubic splines are placed at the 1st quantile, median, and 3rd quartile of the FCAL measurement times for the study cohort. This corresponds to [0.63, 1.64, 3.05] years from diagnosis.\n\nCodengroups <- c(2, 3, 4, 5, 6)\nrep <- 50\nmaxiter <- 10\nif (!file.exists(\"cache/cubicbf.fits.RDS\") | !cache.models){\n  m1 <-  hlme(fixed =  value ~ ns(time, df = 4),\n              random = ~  ns(time, df = 4),\n              subject = \"id\",\n              data = FCcumulative,\n              verbose = FALSE,\n              var.time = \"time\",\n              maxiter = 8000)\n  print(summary(m1))\n  if (!m1$conv) stop(\"LME did not converge \\n\")\n  \n  hlme.metrics <- matrix(nrow = 0, ncol = 3)\n  colnames(hlme.metrics) <- c(\"maximum log-likelihood\", \"AIC\", \"BIC\")\n  temp <- matrix(c(m1$loglik, m1$AIC, m1$BIC),  nrow = 1)\n  rownames(temp) <- \"1\"\n  hlme.metrics <- rbind(hlme.metrics, temp)\n  \n  \n  cubicbf.fits <- list()\n  cubicbf.fits[[\"group1\"]] <- m1\n\n  for (ngroup in ngroups) {\n    ng <- ngroup\n    cl <- parallel::makeCluster(parallel::detectCores())\n    parallel::clusterExport(cl, \"ng\")\n    hlme.fit <- gridsearch(\n      rep = rep,\n      maxiter = maxiter,\n      minit = m1,\n      cl = cl,\n      hlme(fixed =  value ~ ns(time, df = 4),\n           mixture = ~  ns(time, df = 4),\n           random = ~  ns(time, df = 4),\n           subject = \"id\",\n           ng = ng,\n           data = FCcumulative,\n           verbose = FALSE)\n    )\n    parallel::stopCluster(cl)\n    \n    cubicbf.fits[[paste0(\"group\", ngroup)]] <- hlme.fit\n  \n    if (hlme.fit$conv) {\n      cat(\"Convergence achieved for \", ng, \"subgroups ✅ \\n\")\n    } else {\n      cat(\"Convergence NOT achieved for \", ng, \" subgroups ⚠️ \\n\")\n    }\n    \n    temp <- matrix(c(hlme.fit$loglik, hlme.fit$AIC, hlme.fit$BIC),  nrow = 1)\n    rownames(temp) <- ngroup\n    hlme.metrics <- rbind(hlme.metrics, temp)\n  }\n  saveRDS(cubicbf.fits, \"cache/cubicbf.fits.RDS\")\n  saveRDS(hlme.metrics, \"cache/cubicbf.RDS\")\n} else{\n  cubicbf.fits <- readRDS(\"cache/cubicbf.fits.RDS\")\n  hlme.metrics <- readRDS(\"cache/cubicbf.RDS\" )\n}\n\nm1 <- cubicbf.fits[[1]]\nx <- predictY(m1,\n              newdata = data.frame(time = seq(0, 5, by = 0.01)),\n              var.time = \"time\",\n              draws = TRUE)\npar(mfrow = c(1, 2))\n\nplot(FCcumulative$time,\n     FCcumulative$value,\n     xlab = \"Time\",\n     ylab = \"Log(FCAL)\",\n     main = \"LME with Cubic Natural Splines (Log Scale)\",\n     col = rgb(0,0,0, 0.3))\nlines(x$times[,1], x$pred[,1], col = \"red\")\nlines(x$times[,1], x$pred[,2], col = \"red\", lty = 2) # Conf intervals\nlines(x$times[,1], x$pred[,3], col = \"red\", lty = 2) \n\n# Plot knots as vertical lines\nabline(v = as.numeric(attr(ns(FCcumulative$time, df = 4),\n                           \"knots\")),\n       col = \"blue\",\n       lty = 3)\n \nplot(FCcumulative$time,\n     exp(FCcumulative$value),\n     xlab = \"Time\",\n     ylab = \"FCAL\",\n     main = \"LME with Cubic Natural Splines (Measurement Scale)\",\n     col = rgb(0,0,0, 0.3))\nlines(x$times[, 1], exp(x$pred[, 1]), col = \"red\")\nlines(x$times[, 1], exp(x$pred[, 2]), col = \"red\", lty= 2)\nlines(x$times[, 1], exp(x$pred[, 3]), col = \"red\", lty= 2)\nabline(v = as.numeric(attr(ns(FCcumulative$time, df = 4), \"knots\")),\n       col = \"blue\", lty = 3)\npar(mfrow = c(1,1))\n\n\n\nFigure 3: Linear mixed effects (LME) model fitted to data and used to generate inital values for the grid search method used for LCMMs with G > 2"
  },
  {
    "objectID": "selection.html#model-selection",
    "href": "selection.html#model-selection",
    "title": "Model Selection",
    "section": "Model selection",
    "text": "Model selection\nModel fit\nWe consider two metrics when considering model fit: AIC and BIC which penalises model complexity.\n\nCodegroups.1 <- cubicbf.fits[[1]]\ngroups.2 <- cubicbf.fits[[2]]\ngroups.3 <- cubicbf.fits[[3]]\ngroups.4 <- cubicbf.fits[[4]]\ngroups.5 <- cubicbf.fits[[5]]\ngroups.6 <- cubicbf.fits[[6]]\n\nDTbuild(hlme.metrics,\n        caption = \"Fit Metrics for Natural Cubic Splines Model\")\n\n\n\n\n\nConsidering all of the models above, AIC is most optimal for the G = 5 model and BIC is optimal for the G = 2 model. However, considering an alluvial plot (Figure 4) suggest neither G = 2 nor G = 5 are suitable models. The G = 2 model clearly has additional well defined latent classes not properly represented by just two classes, whilst the G=5 model results in an incredibly small new class. As such, G = 4 may be a more suitable alternative.\nClass discrimination\nAlluvial plots\n\nCodere_label <- function(old.G, new.G, alluvial.df){\n  \n  new.order <- rep(new.G, new.G)\n  old.clusters <- subset(alluvial.df, G == old.G)\n  new.clusters <- subset(alluvial.df, G == new.G)\n\n  for (g in old.G:1) {\n    ids <- subset(old.clusters, class == g)$id\n    for (new.g in 1:new.G) {\n      new.clusters.g <- subset(new.clusters, new.g == class)\n      if (nrow(subset(new.clusters.g, id %in% ids)) > 0.5 * length(ids)) {\n        new.order[new.g] <- g \n      }\n    }\n  }\n  \n  alluvial.df[alluvial.df[, \"G\"] == new.G, \"class\"] <-\n    plyr::mapvalues(alluvial.df[alluvial.df[, \"G\"] == new.G, \"class\"],\n                    from = seq(1, new.G),\n                    new.order)\n  return(alluvial.df)\n}\n\n# convert to alluvial format\nalluvial.df <-cbind(groups.2$pprob[, 1:2], G = 2)\nalluvial.df <-rbind(alluvial.df, cbind(groups.3$pprob[, 1:2], G = 3))\nalluvial.df <-rbind(alluvial.df, cbind(groups.4$pprob[, 1:2], G = 4))\nalluvial.df <-rbind(alluvial.df, cbind(groups.5$pprob[, 1:2], G = 5))\nalluvial.df <-rbind(alluvial.df, cbind(groups.6$pprob[, 1:2], G = 6))\nalluvial.df$id <- as.character(alluvial.df$id)\nalluvial.df$class <- as.factor(alluvial.df$class) \n\n# eliminate label switching\nalluvial.df <- re_label(2, 3, alluvial.df)\nalluvial.df <- re_label(3, 4, alluvial.df)\nalluvial.df <- re_label(4, 5, alluvial.df)\nalluvial.df <- re_label(5, 6, alluvial.df)\n\np <- ggplot(alluvial.df,\n            aes(x = G,\n            stratum = class,\n            alluvium = id,\n            fill = class,\n            label = class)) + \n  scale_x_discrete(expand = c(.1, .1)) +\n  geom_flow()  +\n  geom_stratum(alpha = 0.5) +\n  geom_text(stat = \"stratum\", size = 3) +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  ggtitle(\"Alluvial plot of class membership across G\",\n          \"Crohn's disease inception cohort\") +\n  scale_fill_manual(values = c(\"#e3281f\",\n                               \"#3aa534\",\n                               \"#ff5885\",\n                               \"#fbc926\",\n                               \"#511e9d\",\n                               \"black\")) +\n  xlab(\"Assumed number of latent classes\") + \n  ylab(\"Frequency\")\nprint(p)\n\np <- p + ggtitle(\"\", \"\")\nggsave(\"paper/alluvial.png\", p, width = 8, height = 4.5, units = \"in\")\nggsave(\"paper/alluvial.pdf\", p, width = 8, height = 4.5, units = \"in\")\n\n\n\nFigure 4: Alluvial plot demonstrating how class membership changes as the assumed number of classes increase. The height of each band indicates the size of each class.\n\n\n\n\nPosterior classifications\nAn alternative to co-clustering when considering class discrimination is to consider posterior classification possibilities. From the below data, we can see how these posterior probabilities change as the number of assumed classes increase\n\n\nG = 2\nG = 3\nG = 4\nG = 5\nG = 6\n\n\n\n\nCodepostprob(cubicbf.fits[[2]])\n\n \nPosterior classification: \n  class1 class2\nN  96.00 260.00\n%  26.97  73.03\n \nPosterior classification table: \n     --> mean of posterior probabilities in each class \n        prob1  prob2\nclass1 0.8747 0.1253\nclass2 0.0506 0.9494\n \nPosterior probabilities above a threshold (%): \n         class1 class2\nprob>0.7  80.21  95.38\nprob>0.8  70.83  92.31\nprob>0.9  62.50  83.08\n \n\n\n\n\n\nCodepostprob(cubicbf.fits[[3]])\n\n \nPosterior classification: \n  class1 class2 class3\nN  97.00  204.0  55.00\n%  27.25   57.3  15.45\n \nPosterior classification table: \n     --> mean of posterior probabilities in each class \n        prob1  prob2  prob3\nclass1 0.8678 0.0789 0.0533\nclass2 0.0366 0.8925 0.0710\nclass3 0.0606 0.1298 0.8095\n \nPosterior probabilities above a threshold (%): \n         class1 class2 class3\nprob>0.7  80.41  88.24  70.91\nprob>0.8  68.04  77.45  60.00\nprob>0.9  57.73  65.20  43.64\n \n\n\n\n\n\nCodepostprob(cubicbf.fits[[4]])\n\n \nPosterior classification: \n  class1 class2 class3 class4\nN  15.00  58.00  92.00 191.00\n%   4.21  16.29  25.84  53.65\n \nPosterior classification table: \n     --> mean of posterior probabilities in each class \n        prob1  prob2  prob3  prob4\nclass1 0.7105 0.0549 0.0944 0.1401\nclass2 0.0540 0.7997 0.0484 0.0978\nclass3 0.0707 0.0653 0.8191 0.0449\nclass4 0.0747 0.0618 0.0229 0.8406\n \nPosterior probabilities above a threshold (%): \n         class1 class2 class3 class4\nprob>0.7  53.33  65.52  71.74  78.01\nprob>0.8  46.67  58.62  63.04  64.92\nprob>0.9  13.33  39.66  51.09  53.93\n \n\n\n\n\n\nCodepostprob(cubicbf.fits[[5]])\n\n \nPosterior classification: \n  class1 class2 class3 class4 class5\nN  95.00 191.00   6.00  11.00  53.00\n%  26.69  53.65   1.69   3.09  14.89\n \nPosterior classification table: \n     --> mean of posterior probabilities in each class \n        prob1  prob2  prob3  prob4  prob5\nclass1 0.8229 0.0664 0.0353 0.0336 0.0418\nclass2 0.0291 0.8529 0.0048 0.0645 0.0488\nclass3 0.1424 0.0132 0.8431 0.0012 0.0001\nclass4 0.0209 0.1351 0.0000 0.7499 0.0940\nclass5 0.0663 0.0763 0.0001 0.0572 0.8001\n \nPosterior probabilities above a threshold (%): \n         class1 class2 class3 class4 class5\nprob>0.7  74.74  76.96  66.67  63.64  67.92\nprob>0.8  60.00  66.49  66.67  45.45  54.72\nprob>0.9  47.37  53.40  50.00  36.36  39.62\n \n\n\n\n\n\nCodepostprob(cubicbf.fits[[6]])\n\n \nPosterior classification: \n  class1 class2 class3 class4 class5 class6\nN  48.00   63.0  52.00 117.00  67.00   9.00\n%  13.48   17.7  14.61  32.87  18.82   2.53\n \nPosterior classification table: \n     --> mean of posterior probabilities in each class \n        prob1  prob2  prob3  prob4  prob5  prob6\nclass1 0.8604 0.1241 0.0070 0.0001 0.0043 0.0042\nclass2 0.0798 0.7354 0.0588 0.0092 0.1022 0.0147\nclass3 0.0016 0.0447 0.7671 0.0471 0.0893 0.0501\nclass4 0.0003 0.0067 0.0439 0.7737 0.1381 0.0373\nclass5 0.0022 0.1143 0.0554 0.1256 0.6373 0.0651\nclass6 0.0004 0.0169 0.0223 0.0490 0.1295 0.7818\n \nPosterior probabilities above a threshold (%): \n         class1 class2 class3 class4 class5 class6\nprob>0.7  87.50  57.14  65.38  60.68  32.84  77.78\nprob>0.8  66.67  42.86  53.85  52.99  17.91  55.56\nprob>0.9  58.33  26.98  36.54  41.03  13.43  44.44\n \n\n\n\n\n\n\nCodepprobs.2 <- c()\npprobs.3 <- c()\npprobs.4 <- c()\npprobs.5 <- c()\npprobs.6 <- c()\nfor (i in 1:nrow(cubicbf.fits[[1]]$pprob)){\n  class.2 <- groups.2$pprob[i, 2]\n  pprobs.2 <- c(pprobs.2, groups.2$pprob[i, class.2 + 2 ])\n  class.3 <- groups.3$pprob[i, 2]\n  pprobs.3 <- c(pprobs.3, groups.3$pprob[i, class.3 + 2 ])\n  class.4 <- groups.4$pprob[i, 2]\n  pprobs.4 <- c(pprobs.4, groups.4$pprob[i, class.4 + 2 ])\n  class.5 <- groups.5$pprob[i, 2]\n  pprobs.5 <- c(pprobs.5, groups.5$pprob[i, class.5 + 2 ])\n  class.6 <- groups.6$pprob[i, 2]\n  pprobs.6 <- c(pprobs.6, groups.6$pprob[i, class.6 + 2 ])\n}\npprobs.2 <- tibble(prob = pprobs.2)\npprobs.3 <- tibble(prob = pprobs.3)\npprobs.4 <- tibble(prob = pprobs.4)\npprobs.5 <- tibble(prob = pprobs.5)\npprobs.6 <- tibble(prob = pprobs.6)\n\npprobs.2$Model <- as.factor(rep(\"Two classes\", nrow(pprobs.2)))\npprobs.3$Model <- as.factor(rep(\"Three classes\", nrow(pprobs.3)))\npprobs.4$Model <- as.factor(rep(\"Four classes\", nrow(pprobs.4)))\npprobs.5$Model <- as.factor(rep(\"Five classes\", nrow(pprobs.5)))\npprobs.6$Model <- as.factor(rep(\"Six classes\", nrow(pprobs.6)))\npprobs <- rbind(pprobs.2, pprobs.3, pprobs.4, pprobs.5, pprobs.6)\n\np <- pprobs %>%\n  ggplot(aes(x = prob, y = Model)) +\n  #geom_histogram(bins = 40, fill = NA, position=\"identity\")\n  stat_slab(aes(fill = Model),color = \"gray\",\n                    size = 0.8,\n                    alpha = 0.2) +\n  geom_dots(aes(fill = Model, color = Model), dotsize = 1) +\n  xlab(\"Posterior Probability for Class Membership\") +\n  ylab(\"\") + \n  ggtitle(\"Distribution of Posterior Probabilities Across Models\",\n          \"Subject-specific posterior probabilities for assigned class\") +\n  theme_minimal() + \n  scale_color_manual(values = c(\"#e3281f\",\n                                \"#3aa534\",\n                                \"#ff5885\",\n                                \"#fbc926\",\n                                \"#511e9d\")\n                     ) +\n  scale_fill_manual(values = c(\"#e3281f\",\n                               \"#3aa534\",\n                               \"#ff5885\",\n                               \"#fbc926\",\n                               \"#511e9d\")\n                    ) +\n  scale_y_discrete(limits = rev)\nprint(p)\n\n\n\nCodeggsave(\"plots/Distributions.png\", p, width = 8.5, height = 4.5, units = \"in\")\nggsave(\"plots/Distributions.pdf\", p, width = 8.5, height = 4.5, units = \"in\")\n\n\nResidual plots\nTo ensure model assumptions are not violated, residual plots are also consulted. The residual plots are very similar across all models considered and are reassuring.\n\n\nG = 1\nG = 2\nG = 3\nG = 4\nG = 5\nG = 6\n\n\n\n\nCodeplot(cubicbf.fits[[1]], shades = TRUE)\n\n\n\n\n\n\n\nCodeplot(cubicbf.fits[[2]], shades = TRUE)\n\n\n\n\n\n\n\nCodeplot(cubicbf.fits[[3]], shades = TRUE)\n\n\n\n\n\n\n\nCodeplot(cubicbf.fits[[4]], shades = TRUE)\n\n\n\n\n\n\n\nCodeplot(cubicbf.fits[[5]], shades = TRUE)\n\n\n\n\n\n\n\nCodeplot(cubicbf.fits[[6]], shades = TRUE)\n\n\n\n\n\n\n\nSpaghetti plots per class\nPlotting the mean class trajectories alongside spaghetti plots of all subject trajectories provides evidence for the G = 4 model being the most appropriate.\n\n\nFor G = 2\nFor G = 3\nFor G = 4\nFor G = 5\nFor G = 6\n\n\n\nLog-scale, all subjects\n\nCodespaghetti_plot(FCcumulative, cubicbf.fits, G = 2, log = TRUE, sizes = TRUE)\n\nThere are 96 subjects in class 1.\n\n\nThere are 260 subjects in class 2.\n\n\n\n\n\nMeasurement-scale, all subjects\n\nCodespaghetti_plot(FCcumulative, cubicbf.fits, G = 2, log = FALSE)\n\n\n\n\nLog-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 2,\n               log = TRUE,\n               pprob.cutoff = 0.8)\n\n\n\n\nMeasurement-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 2,\n               log = FALSE,\n               pprob.cutoff = 0.8)\n\n\n\n\n\n\n\nLog-scale, all subjects\n\nCode#| fig-width: 11\n#| fig.height: 8\nspaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 3,\n               log = TRUE,\n               mapping = c(1,3,2),\n               sizes = TRUE)\n\nThere are 97 subjects in class 1.\n\n\nThere are 204 subjects in class 3.\n\n\nThere are 55 subjects in class 2.\n\n\n\n\n\nMeasurement-scale, all subjects\n\nCodespaghetti_plot(FCcumulative, cubicbf.fits, G = 3, log = FALSE)\n\n\n\n\nLog-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 3,\n               log = TRUE,\n               pprob.cutoff = 0.8)\n\n\n\n\nMeasurement-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 3,\n               log = FALSE,\n               pprob.cutoff = 0.8)\n\n\n\n\n\n\n\nClass membership has been relabelled to ensure consistency with the alluvial plot\nLog-scale, all subjects\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 4,\n               log = TRUE,\n               mapping = c(3,2,4,1),\n               sizes = TRUE)\n\nThere are 191 subjects in class 3.\n\n\nThere are 58 subjects in class 2.\n\n\nThere are 15 subjects in class 4.\n\n\nThere are 92 subjects in class 1.\n\n\n\n\n\nMeasurement-scale, all subjects\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 4,\n               log = FALSE,\n               mapping = c(3, 2, 4, 1))\n\n\n\n\nLog-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 4,\n               log = TRUE,\n               pprob.cutoff = 0.8,\n               mapping = c(3, 2, 4, 1))\n\n\n\n\nMeasurement-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 4,\n               log = FALSE,\n               pprob.cutoff = 0.8,\n               mapping = c(3, 2, 4, 1))\n\n\n\n\n\n\n\nLog-scale, all subjects\n\nCodespaghetti_plot(FCcumulative, cubicbf.fits, G = 5, log = TRUE, sizes = TRUE)\n\n\n\n\nMeasurement-scale, all subjects\n\nCodespaghetti_plot(FCcumulative, cubicbf.fits, G = 5, log = FALSE)\n\n\n\n\nLog-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 5,\n               log = TRUE,\n               pprob.cutoff = 0.8)\n\n\n\n\nMeasurement-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 5,\n               log = FALSE,\n               pprob.cutoff = 0.8)\n\n\n\n\n\n\n\nLog-scale, all subjects\n\nCodespaghetti_plot(FCcumulative, cubicbf.fits, G = 6, log = TRUE, sizes = TRUE)\n\nThere are 48 subjects in class 1.\n\n\nThere are 63 subjects in class 2.\n\n\nThere are 52 subjects in class 3.\n\n\nThere are 117 subjects in class 4.\n\n\nThere are 67 subjects in class 5.\n\n\nThere are 9 subjects in class 6.\n\n\n\n\n\nMeasurement-scale, all subjects\n\nCodespaghetti_plot(FCcumulative, cubicbf.fits, G = 6, log = FALSE)\n\n\n\n\nLog-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 6,\n               log = TRUE,\n               pprob.cutoff = 0.8)\n\n\n\n\nMeasurement-scale, pprob > 0.8 only\n\nCodespaghetti_plot(FCcumulative,\n               cubicbf.fits,\n               G = 6,\n               log = FALSE,\n               pprob.cutoff = 0.8)\n\n\n\n\n\n\n\n\nModel output\nAfter considering all of the above findings, the model which assumes four latent classes has been deemed the most appropriate. The summary statistics for this model can be found below. The four splines are denoted by X_{1}(t), \\ldots, X_{4}(t). We use this notation in the supplementary materials for the paper.\n\nCode### Match notation to supp. materials\nx <- cubicbf.fits[[4]]\nx$Xnames <- c(\"Intercept\", paste0(\"X\", seq(1, 4), \"(t)\")) # Random effects\nnames(x$best) <- c(paste(\"Intercept class \", seq(1, 3)),# Class membership model\n                   paste(\"Intercept class \", seq(1, 4)),# Longitudinal model\n                   paste(\"X1(t) class \", seq (1, 4)),\n                   paste(\"X2(t) class \", seq (1, 4)),\n                   paste(\"X3(t) class \", seq (1, 4)),\n                   paste(\"X4(t) class \", seq (1, 4)),\n                   paste(\"Varcov \", seq(1, 15)),\n                   \"Standard error\")\nsummary(x)\n\nHeterogenous linear mixed model \n     fitted by maximum likelihood method \n \nhlme(fixed = value ~ ns(time, df = 4), mixture = ~ns(time, df = 4), \n    random = ~ns(time, df = 4), subject = \"id\", ng = ng, data = FCcumulative, \n    verbose = FALSE)\n \nStatistical Model: \n     Dataset: FCcumulative \n     Number of subjects: 356 \n     Number of observations: 2856 \n     Number of latent classes: 4 \n     Number of parameters: 39  \n \nIteration process: \n     Convergence criteria satisfied \n     Number of iterations:  20 \n     Convergence criteria: parameters= 8.7e-05 \n                         : likelihood= 8.1e-05 \n                         : second derivatives= 1.8e-07 \n \nGoodness-of-fit statistics: \n     maximum log-likelihood: -3983.49  \n     AIC: 8044.97  \n     BIC: 8196.1  \n \n \nMaximum Likelihood Estimates: \n \nFixed effects in the class-membership model:\n(the class of reference is the last class) \n\n                       coef      Se    Wald p-value\nIntercept class  1 -1.60714 0.60509  -2.656 0.00791\nIntercept class  2 -0.97547 0.24466  -3.987 0.00007\nIntercept class  3 -0.71987 0.19000  -3.789 0.00015\n\nFixed effects in the longitudinal model:\n\n                       coef      Se    Wald p-value\nIntercept class  1  6.63001 0.18260  36.309 0.00000\nIntercept class  2  6.60767 0.10692  61.803 0.00000\nIntercept class  3  6.40075 0.08792  72.805 0.00000\nIntercept class  4  6.68268 0.05943 112.452 0.00000\nX1(t) class  1     -0.06345 0.61580  -0.103 0.91794\nX1(t) class  2     -3.02786 0.30213 -10.022 0.00000\nX1(t) class  3     -2.58120 0.28225  -9.145 0.00000\nX1(t) class  4     -0.32661 0.15224  -2.145 0.03192\nX2(t) class  1     -1.66557 0.66716  -2.497 0.01254\nX2(t) class  2     -2.43912 0.41567  -5.868 0.00000\nX2(t) class  3     -0.73639 0.30260  -2.434 0.01495\nX2(t) class  4     -0.76938 0.18402  -4.181 0.00003\nX3(t) class  1     -4.31021 0.83955  -5.134 0.00000\nX3(t) class  2     -2.38786 0.33995  -7.024 0.00000\nX3(t) class  3     -4.60973 0.32173 -14.328 0.00000\nX3(t) class  4     -1.29125 0.19433  -6.645 0.00000\nX4(t) class  1     -1.93148 0.69759  -2.769 0.00563\nX4(t) class  2     -2.65713 0.33911  -7.835 0.00000\nX4(t) class  3     -0.59806 0.28348  -2.110 0.03488\nX4(t) class  4     -0.21947 0.15304  -1.434 0.15155\n\n\nVariance-covariance matrix of the random-effects:\n          Intercept    X1(t)   X2(t)   X3(t)   X4(t)\nIntercept   0.01816                                 \nX1(t)       0.00354  0.69013                        \nX2(t)      -0.07139 -0.20182 1.56220                \nX3(t)       0.03241 -0.21967 0.67262 0.65261        \nX4(t)      -0.06129 -0.19956 0.31425 0.09602 0.47886\n\n                             coef      Se\nResidual standard error:  0.77672 0.01273"
  },
  {
    "objectID": "selection.html#session-information",
    "href": "selection.html#session-information",
    "title": "Model Selection",
    "section": "Session information",
    "text": "Session information\n\n\nR version 4.2.0 (2022-04-22)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_GB.UTF-8||en_GB.UTF-8||en_GB.UTF-8||C||en_GB.UTF-8||en_GB.UTF-8\nattached base packages: grid, splines, parallel, stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: DT(v.0.22), ggalluvial(v.0.12.3), ggdist(v.3.1.1), patchwork(v.1.1.1), htmltools(v.0.5.2), lcmm(v.1.9.5), randtoolbox(v.1.31.1), rngWELL(v.0.10-7), mvtnorm(v.1.1-3), survival(v.3.3-1), forcats(v.0.5.1), stringr(v.1.4.0), dplyr(v.1.0.9), purrr(v.0.3.4), readr(v.2.1.2), tidyr(v.1.2.0), tibble(v.3.1.7), ggplot2(v.3.3.6) and tidyverse(v.1.3.1)\nloaded via a namespace (and not attached): httr(v.1.4.3), jsonlite(v.1.8.0), modelr(v.0.1.8), assertthat(v.0.2.1), distributional(v.0.3.0), pander(v.0.6.5), renv(v.0.15.5), cellranger(v.1.1.0), yaml(v.2.3.5), pillar(v.1.7.0), backports(v.1.4.1), lattice(v.0.20-45), glue(v.1.6.2), digest(v.0.6.29), rvest(v.1.0.2), colorspace(v.2.0-3), plyr(v.1.8.7), Matrix(v.1.4-1), pkgconfig(v.2.0.3), broom(v.0.8.0), haven(v.2.5.0), scales(v.1.2.0), tzdb(v.0.3.0), generics(v.0.1.2), farver(v.2.1.0), ellipsis(v.0.3.2), withr(v.2.5.0), cli(v.3.3.0), magrittr(v.2.0.3), crayon(v.1.5.1), readxl(v.1.4.0), evaluate(v.0.15), fs(v.1.5.2), fansi(v.1.0.3), nlme(v.3.1-157), xml2(v.1.3.3), textshaping(v.0.3.6), tools(v.4.2.0), hms(v.1.1.1), lifecycle(v.1.0.1), munsell(v.0.5.0), reprex(v.2.0.1), jquerylib(v.0.1.4), compiler(v.4.2.0), systemfonts(v.1.0.4), rlang(v.1.0.2), rstudioapi(v.0.13), htmlwidgets(v.1.5.4), crosstalk(v.1.2.0), labeling(v.0.4.2), rmarkdown(v.2.14), gtable(v.0.3.0), DBI(v.1.1.2), R6(v.2.5.1), lubridate(v.1.8.0), knitr(v.1.39), fastmap(v.1.1.0), utf8(v.1.2.2), ragg(v.1.2.2), stringi(v.1.7.6), Rcpp(v.1.0.8.3), vctrs(v.0.4.1), dbplyr(v.2.1.1), tidyselect(v.1.1.2) and xfun(v.0.30)"
  },
  {
    "objectID": "selection.html#acknowledgments",
    "href": "selection.html#acknowledgments",
    "title": "Model Selection",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis work is funded by the Medical Research Council & University of Edinburgh via a Precision Medicine PhD studentship (MR/N013166/1, to NC-C)"
  },
  {
    "objectID": "selection.html#author-contributions",
    "href": "selection.html#author-contributions",
    "title": "Model Selection",
    "section": "Author contributions",
    "text": "Author contributions\nNC-C wrote the analysis. KM and CAV performed code review and contributed suggestions. KM, RM and CAV provided feedback."
  },
  {
    "objectID": "selection.html#reuse",
    "href": "selection.html#reuse",
    "title": "Model Selection",
    "section": "Reuse",
    "text": "Reuse\nLicensed by CC BY  unless otherwise stated."
  },
  {
    "objectID": "associations.html",
    "href": "associations.html",
    "title": "Association Testing",
    "section": "",
    "text": "Codeset.seed(123)\nlibrary(nnet) # Multinomial logistic regression\nlibrary(tidymodels) # Modelling framework\nlibrary(ranger) # Random forest\nlibrary(survival) # Kaplan-Meier\nlibrary(survminer) # Plot Kaplan-Meier curves\nlibrary(patchwork)\nlibrary(vip)\n\nif (!require(plyr)) {s\n  install.packages(\"plyr\")\n}\n\n########################\n#-- Custom Functions --#\n########################\n\n#' @title Calculate descriptive statistics for non-normal continuous variables\n#' @param vars Character vector of continuous variable names\n#' @param data Dataframe object\n#' @description\n#' Finds the median, first quartile, third quartile, minimum and maximum\n#'   statistics for non-normal continuous variables\n#' @return A dataframe object with length(vars) rows and 7 columns\n#' corresponding to the descriptive statistics calculated\ndescribe_cont <- function(vars, data){\n  if (tibble::is_tibble(data)) {\n    data <- as.data.frame(data)\n  }\n  out_desc <- data.frame(Variable = character(),\n                         n = double(),\n                         Median = double(),\n                         Q1 = double(),\n                         Q3 = double(),\n                         Min = double(),\n                         Max = double()\n  )\n  for (i in 1:length(vars)) {\n    var <- vars[i]\n    med <- summary(data[, var])[c(3, 2, 5, 1, 6)]\n    n <- sum(!is.na(data[, var]))\n    out_desc[i, ] <- c(var, n, signif(med, 3))\n  }\n  out_desc\n}\n#' @title Calculate descriptive statistics for categorical variables\n#' @param vars Character vector of categorical variable names\n#' @param data Dataframe object\n#' @description\n#' Prints frequency and proportional tables for given categorical variables\n#' @return A dataframe object with length(vars) rows and 3 columns\n#' corresponding to the descriptive statistics calculated\ndescribe_cat <- function(vars, data){\n  if (tibble::is_tibble(data)) {\n    data <- as.data.frame(data)\n  }\n  for (i in 1:length(vars)) {\n    var <- vars[i]\n    print(table(data[, var], useNA = \"always\"))\n    print(prop.table(table(data[, var], useNA = \"always\")))\n  }\n}\n\nkaplan_plot <- function(model, title, subtitle = TRUE, legend = TRUE){\n  if (legend) {\n    p <- ggsurvplot(model,\n                    conf.int = TRUE,\n                    surv.median.line = \"none\",\n                    data = demographic,\n                    pval = TRUE,\n                    pval.method = TRUE,\n                    ggtheme = theme_minimal() +\n                      theme(plot.title = element_text(face = \"bold\",\n                                                      size = 20)\n                            ),\n                    pval.size = 4,\n                    legend = \"bottom\")\n  } else {\n        p <- ggsurvplot(model,\n                    conf.int = TRUE,\n                    surv.median.line = \"none\",\n                    data = demographic,\n                    pval = TRUE,\n                    pval.method = TRUE,\n                    ggtheme = theme_minimal() + \n                      theme(plot.title = element_text(face = \"bold\",\n                                                      size = 20)\n                            ),\n                    pval.size = 4,\n                    legend = \"none\")\n  }\n  if (subtitle) {\n    p <- p + ggtitle(title, \"Stratified by class\") + xlab(\"Time (years)\") \n  } else {\n    p <- p + ggtitle(title) + xlab(\"Time (years)\")\n  }\n  return(p)\n} \n\n\nAfter deeming the four-class LCMM to be the most suitable, we will now explore potential associations with class membership. We will consider variables typically available at diagnosis, outcomes usually indicative of a poor disease course, and treatments prescribed within one year of diagnosis. For all categorical variables, frequency tables for the study population and classes have been generated in addition to either chi-squared test or (if a cell has < 5 observations) Fisher’s exact test results. For continuous variables, the median, first quartile, and third quartile have been reported for the study population and classes in addition to ANOVA results.\nTo streamline the analysis, descriptive functions have been created, describe_cat() and describe_cont() which generate the aforementioned results\nWe also attempt to predict class membership based on these variables to determine if class membership can be accurately predicted ahead of time using these variables. We consider both multinomial logistic regression and random forest predictive models for this purpose."
  },
  {
    "objectID": "associations.html#outcomes",
    "href": "associations.html#outcomes",
    "title": "Association Testing",
    "section": "Outcomes",
    "text": "Outcomes\nWe will first consider associations with outcomes in order to determine if there is are potentially clinically significant aspects to these classes. For this purpose, we use Kaplan-Meier curves for endpoints of interest stratified by class membership. The endpoints considered are hospitalisation, surgery, disease progression, and a composite end point. We use log-rank tests to determine if survival distributions differ across classes. All censoring indicators have been mapped to 0 (censored) and 1 (observed). Patients who met one of these endpoints within one year of diagnosis were excluded from the Crohn’s disease inception cohort and therefore no endpoints are observed within one year of diagnosis for the study cohort. For more information, see the Data Cleaning Section.\nWe have found the survival distributions for hospitalisation, disease progression, and the composite endpoint to be significantly different across classes but not the survival distribution for the surgery endpoint.\nFrom the below plots, we can clearly see class 1, which is characterised by a decrease in FCAL, results in high survival probabilities over time. Class 2, distinguished by consistently high FCAL, demonstrates poor survival probabilities.\n\nCodeFCcumulative <- readRDS(paste0(\"/Volumes/igmm/cvallejo-predicct/processed-data/cdi/\",\n                               \"FCcumulativeLongInc.RDS\"))\n\nmodel <- readRDS(\"cache/cubicbf.fits.RDS\")[[4]]\nmodel$pprob$class <- plyr::mapvalues(model$pprob$class,\n                                     from = c(4,3,2,1),\n                                     to = c(2,1,3,4))\ndemographic <- read.csv(paste0(\"/Volumes/igmm/cvallejo-predicct/data/\",\n                               \"crohns-inception/20220328/\",\n                               \"Nathan_FCprogressioncohort.csv\")\n                         )\ndemographic <- subset(demographic, Number %in% model$pprob$id)\n\ncolnames(demographic)[1] <- \"id\"\nclasses <- model$pprob[, 1:2]\n\ndemographic <- merge(demographic, classes, by = \"id\")\n\n\n\n\nHospitalisation\nSurgery\nDisease progression\nComposite endpoint\n\n\n\n\nCodehosp.censor <- c()\nhosp.time <- c()\n\nfor (i in 1:nrow(demographic)) {\n  subject <- subset(FCcumulative, id == demographic[i, \"id\"])\n  hosp.censor <- c(hosp.censor, subject[1, \"hosp\"])\n  hosp.time <- c(hosp.time, subject[1, \"hosp.date\"])\n}\n\ndemographic$hosp.censor <- hosp.censor\ndemographic$hosp.time <- hosp.time\n\nhosp.model <- survfit(Surv(hosp.time, hosp.censor) ~ class, data = demographic)\n\nkaplan_plot(hosp.model, \"Kaplan-Meier Curves for Hospitalisation\")\n\n\n\n\n\n\n\nCodesurg.censor <- c()\nsurg.time <- c()\n\nfor (i in 1:nrow(demographic)) {\n  subject <- subset(FCcumulative, id == demographic[i, \"id\"])\n  surg.censor <- c(surg.censor, subject[1, \"surg\"])\n  surg.time <- c(surg.time, subject[1, \"surg.date\"])\n}\n\ndemographic$surg.censor <- surg.censor\ndemographic$surg.time <- surg.time\n\nsurg.model <- survfit(Surv(surg.time, surg.censor) ~ class, data = demographic)\n\nkaplan_plot(surg.model, \"Kaplan-Meier Curves for Surgery\")\n\n\n\n\n\n\n\nCodeprog.censor <- c()\nprog.time <- c()\n\nfor (i in 1:nrow(demographic)) {\n  subject <- subset(FCcumulative, id == demographic[i, \"id\"])\n  prog.censor <- c(prog.censor, subject[1, \"prog\"])\n  prog.time <- c(prog.time, subject[1, \"prog.date\"])\n}\n\ndemographic$prog.censor <- prog.censor\ndemographic$prog.time <- prog.time\n\nprog.model <- survfit(Surv(prog.time, prog.censor) ~ class, data = demographic)\n\nkaplan_plot(prog.model, \"Kaplan-Meier Curves for Disease Progression\")\n\n\n\n\n\n\n\nCodecomposite.censor <- c()\ncomposite.time <- c()\n\nfor (i in 1:nrow(demographic)) {\n  subject <- subset(FCcumulative, id == demographic[i, \"id\"])\n  composite.censor <- c(composite.censor, subject[1, \"composite\"])\n  composite.time <- c(composite.time, subject[1, \"composites.day\"])\n}\n\ndemographic$composite.censor <- composite.censor\ndemographic$composite.time <- composite.time / 365.25\n\ncomp.model <- survfit(Surv(composite.time, composite.censor) ~ class,\n                      data = demographic)\nkaplan_plot(comp.model, \"Kaplan-Meier Curves for Composite Endpoint\")"
  },
  {
    "objectID": "associations.html#characteristics-available-at-diagnosis",
    "href": "associations.html#characteristics-available-at-diagnosis",
    "title": "Association Testing",
    "section": "Characteristics available at diagnosis",
    "text": "Characteristics available at diagnosis\nWe have found two variables available at diagnosis to be associated with class membership: smoking at diagnosis (p = 0.015) and upper gastrointestinal inflammation (Montreal L4; p < 0.001).\n\n\nSex\nAge at diagnosis\nSmoking at diagnosis\nDiagnostic FCAL\nMontreal behaviour\nPerianal\nMontreal location\nUpper GI\n\n\n\nPopulation\n\nCodedescribe_cat(\"SEX\", demographic)\n\n\n   F    M <NA> \n 173  183    0 \n\n        F         M      <NA> \n0.4859551 0.5140449 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"SEX\", subset(demographic, class == 1))\n\n\n   F    M <NA> \n  49   43    0 \n\n        F         M      <NA> \n0.5326087 0.4673913 0.0000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"SEX\", subset(demographic, class == 2))\n\n\n   F    M <NA> \n  84  107    0 \n\n        F         M      <NA> \n0.4397906 0.5602094 0.0000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"SEX\", subset(demographic, class == 3))\n\n\n   F    M <NA> \n  34   24    0 \n\n        F         M      <NA> \n0.5862069 0.4137931 0.0000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"SEX\", subset(demographic, class == 4))\n\n\n   F    M <NA> \n   6    9    0 \n\n   F    M <NA> \n 0.4  0.6  0.0 \n\n\nChi-squared test\n\nCodechisq.test(demographic$SEX, demographic$class)\n\n\n    Pearson's Chi-squared test\n\ndata:  demographic$SEX and demographic$class\nX-squared = 5.2083, df = 3, p-value = 0.1572\n\n\n\n\n\nPopulation\n\nCodedescribe_cont(\"age\", demographic)\n\n  Variable   n Median Q1   Q3  Min  Max\n1      age 356   27.3 16 48.7 3.75 87.1\n\n\nClass = 1\n\nCodedescribe_cont(\"age\", subset(demographic, class == 1))\n\n  Variable  n Median   Q1   Q3  Min  Max\n1      age 92   29.6 20.4 45.3 6.87 87.1\n\n\nClass = 2\n\nCodedescribe_cont(\"age\", subset(demographic, class == 2))\n\n  Variable   n Median   Q1   Q3  Min  Max\n1      age 191   26.4 15.3 49.9 3.75 81.3\n\n\nClass = 3\n\nCodedescribe_cont(\"age\", subset(demographic, class == 3))\n\n  Variable  n Median   Q1   Q3  Min  Max\n1      age 58   26.8 15.1 50.9 4.98 78.4\n\n\nClass = 4\n\nCodedescribe_cont(\"age\", subset(demographic, class == 4))\n\n  Variable  n Median   Q1   Q3  Min  Max\n1      age 15   21.7 14.5 51.2 10.5 71.3\n\n\nANOVA\n\nCodesummary(aov(age ~ class, data = demographic))\n\n             Df Sum Sq Mean Sq F value Pr(>F)\nclass         1     14    14.5   0.036  0.851\nResiduals   354 144163   407.2               \n\n\n\n\n\nIf a participant has reported to have not smoked in the past, then it is assumed the participant did not smoke at IBD diagnosis. Otherwise smoking status is taken from the participant’s response to being asked if they were smoking when diagnosed with IBD.\n\nCodecolnames(demographic)[colnames(demographic) == \"SMOKER..Y.N.\"] <- \"smoking\"\ndemographic$smoking <- factor(demographic$smoking, labels = c(\"no\", \"yes\"))\n\n\nPopulation\n\nCodedescribe_cat(\"smoking\", demographic)\n\n\n  no  yes <NA> \n 286   70    0 \n\n       no       yes      <NA> \n0.8033708 0.1966292 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"smoking\", subset(demographic, class == 1))\n\n\n  no  yes <NA> \n  70   22    0 \n\n       no       yes      <NA> \n0.7608696 0.2391304 0.0000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"smoking\", subset(demographic, class == 2))\n\n\n  no  yes <NA> \n 148   43    0 \n\n       no       yes      <NA> \n0.7748691 0.2251309 0.0000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"smoking\", subset(demographic, class == 3))\n\n\n  no  yes <NA> \n  54    4    0 \n\n        no        yes       <NA> \n0.93103448 0.06896552 0.00000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"smoking\", subset(demographic, class == 4))\n\n\n  no  yes <NA> \n  14    1    0 \n\n        no        yes       <NA> \n0.93333333 0.06666667 0.00000000 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$smoking)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$smoking\np-value = 0.01455\nalternative hypothesis: two.sided\n\n\n\n\n\nPopulation\n\nCodeFCcumulative <- readRDS(paste0(\"/Volumes/igmm/cvallejo-predicct/processed-data/cdi/\",\n                               \"FCcumulativeLongInc.RDS\"))\nFCcumulative <- FCcumulative[order(FCcumulative$time), ]\nDiagFC <- rep(NA, nrow(demographic))\nfor (i in 1:nrow(demographic)) {\n  subject <- demographic[i, \"id\"]\n  subject.data <- subset(FCcumulative, id == subject)\n  DiagFC[i] <- subject.data[1, \"value\"] # Already sorted by time\n}\ndemographic$DiagFC <- DiagFC\n\ndescribe_cont(\"DiagFC\", demographic)\n\n  Variable   n Median  Q1   Q3 Min  Max\n1   DiagFC 356    820 590 1140 250 2500\n\n\nClass = 1\n\nCodedescribe_cont(\"DiagFC\", subset(demographic, class == 1))\n\n  Variable  n Median  Q1  Q3 Min  Max\n1   DiagFC 92    725 500 986 253 2500\n\n\nClass = 2\n\nCodedescribe_cont(\"DiagFC\", subset(demographic, class == 2))\n\n  Variable   n Median  Q1   Q3 Min  Max\n1   DiagFC 191    900 610 1270 250 2500\n\n\nClass = 3\n\nCodedescribe_cont(\"DiagFC\", subset(demographic, class == 3))\n\n  Variable  n Median  Q1   Q3 Min  Max\n1   DiagFC 58    825 592 1180 310 2500\n\n\nClass = 4\n\nCodedescribe_cont(\"DiagFC\", subset(demographic, class == 4))\n\n  Variable  n Median  Q1   Q3 Min  Max\n1   DiagFC 15    660 630 1160 470 2040\n\n\nANOVA\n\nCodesummary(aov(DiagFC ~ class, data = demographic))\n\n             Df    Sum Sq Mean Sq F value Pr(>F)\nclass         1    676441  676441   2.295  0.131\nResiduals   354 104332274  294724               \n\n\n\n\n\n\nCodecolnames(demographic)[colnames(demographic) == \"BEHAVIOUR.AT.DIAGNOSIS\"] <- \"behaviour\"\ndemographic$behaviour <- as.factor(demographic$behaviour)\n\n\nPopulation\n\nCodedescribe_cat(\"behaviour\", demographic)\n\n\n   1    2    3 <NA> \n 323   30    3    0 \n\n          1           2           3        <NA> \n0.907303371 0.084269663 0.008426966 0.000000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"behaviour\", subset(demographic, class == 1))\n\n\n   1    2    3 <NA> \n  80   10    2    0 \n\n         1          2          3       <NA> \n0.86956522 0.10869565 0.02173913 0.00000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"behaviour\", subset(demographic, class == 2))\n\n\n   1    2    3 <NA> \n 175   15    1    0 \n\n          1           2           3        <NA> \n0.916230366 0.078534031 0.005235602 0.000000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"behaviour\", subset(demographic, class == 3))\n\n\n   1    2    3 <NA> \n  55    3    0    0 \n\n         1          2          3       <NA> \n0.94827586 0.05172414 0.00000000 0.00000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"behaviour\", subset(demographic, class == 4))\n\n\n   1    2    3 <NA> \n  13    2    0    0 \n\n        1         2         3      <NA> \n0.8666667 0.1333333 0.0000000 0.0000000 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$behaviour)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$behaviour\np-value = 0.4937\nalternative hypothesis: two.sided\n\n\n\n\n\n\nCodecolnames(demographic)[colnames(demographic) == \"PERIANAL.DISEASE..Y.N.\"] <- \"peri\"\ndemographic$peri <- factor(demographic$peri, labels = c(\"no\", \"yes\"))\n\n\nPopulation\n\nCodedescribe_cat(\"peri\", demographic)\n\n\n  no  yes <NA> \n 299   57    0 \n\n       no       yes      <NA> \n0.8398876 0.1601124 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"peri\", subset(demographic, class == 1))\n\n\n  no  yes <NA> \n  75   17    0 \n\n       no       yes      <NA> \n0.8152174 0.1847826 0.0000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"peri\", subset(demographic, class == 2))\n\n\n  no  yes <NA> \n 163   28    0 \n\n       no       yes      <NA> \n0.8534031 0.1465969 0.0000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"peri\", subset(demographic, class == 3))\n\n\n  no  yes <NA> \n  49    9    0 \n\n       no       yes      <NA> \n0.8448276 0.1551724 0.0000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"peri\", subset(demographic, class == 4))\n\n\n  no  yes <NA> \n  12    3    0 \n\n  no  yes <NA> \n 0.8  0.2  0.0 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$peri)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$peri\np-value = 0.7757\nalternative hypothesis: two.sided\n\n\n\n\n\n\nCodedemographic$LOCATION <- factor(demographic$LOCATION, labels = c(\"L1\", \"L2\", \"L3\"))\n\n\nPopulation\n\nCodedescribe_cat(\"LOCATION\", demographic)\n\n\n  L1   L2   L3 <NA> \n  95  140  121    0 \n\n       L1        L2        L3      <NA> \n0.2668539 0.3932584 0.3398876 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"LOCATION\", subset(demographic, class == 1))\n\n\n  L1   L2   L3 <NA> \n  22   39   31    0 \n\n       L1        L2        L3      <NA> \n0.2391304 0.4239130 0.3369565 0.0000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"LOCATION\", subset(demographic, class == 2))\n\n\n  L1   L2   L3 <NA> \n  58   68   65    0 \n\n       L1        L2        L3      <NA> \n0.3036649 0.3560209 0.3403141 0.0000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"LOCATION\", subset(demographic, class == 3))\n\n\n  L1   L2   L3 <NA> \n   9   30   19    0 \n\n       L1        L2        L3      <NA> \n0.1551724 0.5172414 0.3275862 0.0000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"LOCATION\", subset(demographic, class == 4))\n\n\n  L1   L2   L3 <NA> \n   6    3    6    0 \n\n  L1   L2   L3 <NA> \n 0.4  0.2  0.4  0.0 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$LOCATION, workspace = 800000)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$LOCATION\np-value = 0.125\nalternative hypothesis: two.sided\n\n\n\n\n\n\nCodecolnames(demographic)[colnames(demographic) == \"L4.Modifier\"] <- \"L4\"\ndemographic$L4 <- factor(demographic$L4, labels = c(\"no\", \"yes\"))\n\n\nPopulation\n\nCodedescribe_cat(\"L4\", demographic)\n\n\n  no  yes <NA> \n 272   84    0 \n\n       no       yes      <NA> \n0.7640449 0.2359551 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"L4\", subset(demographic, class == 1))\n\n\n  no  yes <NA> \n  84    8    0 \n\n        no        yes       <NA> \n0.91304348 0.08695652 0.00000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"L4\", subset(demographic, class == 2))\n\n\n  no  yes <NA> \n 140   51    0 \n\n       no       yes      <NA> \n0.7329843 0.2670157 0.0000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"L4\", subset(demographic, class == 3))\n\n\n  no  yes <NA> \n  38   20    0 \n\n       no       yes      <NA> \n0.6551724 0.3448276 0.0000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"L4\", subset(demographic, class == 4))\n\n\n  no  yes <NA> \n  10    5    0 \n\n       no       yes      <NA> \n0.6666667 0.3333333 0.0000000 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$L4)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$L4\np-value = 0.0002175\nalternative hypothesis: two.sided"
  },
  {
    "objectID": "associations.html#treatment-effects",
    "href": "associations.html#treatment-effects",
    "title": "Association Testing",
    "section": "Treatment effects",
    "text": "Treatment effects\nTo explore the potential presence of treatment effects, we have tested for associations between class membership and common IBD prescriptions. All treatments were prescribed within one year of diagnosis unless otherwise stated. Thiprine use (p = 0.02), biologic use within three months of diagnosis (p < 0.001), and biologic use within one year of diagnosis (p < 0.001) were found to be significantly associated with class membership.\n\n\n5ASA\nThioprine\nCorticosteroids\nMethotrexate\nEEN\nBiologic use within three months\nBiologic use within one year\nDate of diagnosis\n\n\n\nPopulation\n\nCodedescribe_cat(\"X5ASA\", demographic)\n\n\n   N    Y <NA> \n 280   76    0 \n\n        N         Y      <NA> \n0.7865169 0.2134831 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"X5ASA\", subset(demographic, class == 1))\n\n\n   N    Y <NA> \n  77   15    0 \n\n        N         Y      <NA> \n0.8369565 0.1630435 0.0000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"X5ASA\", subset(demographic, class == 2))\n\n\n   N    Y <NA> \n 143   48    0 \n\n        N         Y      <NA> \n0.7486911 0.2513089 0.0000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"X5ASA\", subset(demographic, class == 3))\n\n\n   N    Y <NA> \n  47   11    0 \n\n        N         Y      <NA> \n0.8103448 0.1896552 0.0000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"X5ASA\", subset(demographic, class == 4))\n\n\n   N    Y <NA> \n  13    2    0 \n\n        N         Y      <NA> \n0.8666667 0.1333333 0.0000000 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$X5ASA)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$X5ASA\np-value = 0.3261\nalternative hypothesis: two.sided\n\n\n\n\n\n\nCodecolnames(demographic)[colnames(demographic) == \"THIOPURINE.WITHIN.1st.YEAR..Y.N.\"] <- \"use_thio\"\n\n\nPopulation\n\nCodedescribe_cat(\"use_thio\", demographic)\n\n\n   N    Y <NA> \n 106  250    0 \n\n        N         Y      <NA> \n0.2977528 0.7022472 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"use_thio\", subset(demographic, class == 1))\n\n\n   N    Y <NA> \n  30   62    0 \n\n       N        Y     <NA> \n0.326087 0.673913 0.000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"use_thio\", subset(demographic, class == 2))\n\n\n   N    Y <NA> \n  64  127    0 \n\n        N         Y      <NA> \n0.3350785 0.6649215 0.0000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"use_thio\", subset(demographic, class == 3))\n\n\n   N    Y <NA> \n   8   50    0 \n\n       N        Y     <NA> \n0.137931 0.862069 0.000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"use_thio\", subset(demographic, class == 4))\n\n\n   N    Y <NA> \n   4   11    0 \n\n        N         Y      <NA> \n0.2666667 0.7333333 0.0000000 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$use_thio)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$use_thio\np-value = 0.02305\nalternative hypothesis: two.sided\n\n\n\n\n\n\nCodecolnames(demographic)[colnames(demographic) == \"CORTICOSTEROIDS.WITHIN.1st.YEAR...Y.N.\"] <- \"use_cortico\"\ndemographic$use_cortico <- plyr::mapvalues(demographic$use_cortico, \"y\", \"Y\")\n\n\nPopulation\n\nCodedescribe_cat(\"use_cortico\", demographic)\n\n\n   N    Y <NA> \n  58  298    0 \n\n        N         Y      <NA> \n0.1629213 0.8370787 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"use_cortico\", subset(demographic, class == 1))\n\n\n   N    Y <NA> \n  14   78    0 \n\n        N         Y      <NA> \n0.1521739 0.8478261 0.0000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"use_cortico\", subset(demographic, class == 2))\n\n\n   N    Y <NA> \n  32  159    0 \n\n        N         Y      <NA> \n0.1675393 0.8324607 0.0000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"use_cortico\", subset(demographic, class == 3))\n\n\n   N    Y <NA> \n  10   48    0 \n\n        N         Y      <NA> \n0.1724138 0.8275862 0.0000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"use_cortico\", subset(demographic, class == 4))\n\n\n   N    Y <NA> \n   2   13    0 \n\n        N         Y      <NA> \n0.1333333 0.8666667 0.0000000 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$use_cortico)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$use_cortico\np-value = 0.9829\nalternative hypothesis: two.sided\n\n\n\n\n\n\nCodecolnames(demographic)[colnames(demographic) == \"MTX.WITHIN.1st.YEAR..Y.N.\"] <- \"use_mtx\"\n\n\nPopulation\n\nCodedescribe_cat(\"use_mtx\", demographic)\n\n\n   N    Y <NA> \n 341   15    0 \n\n         N          Y       <NA> \n0.95786517 0.04213483 0.00000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"use_mtx\", subset(demographic, class == 1))\n\n\n   N    Y <NA> \n  84    8    0 \n\n         N          Y       <NA> \n0.91304348 0.08695652 0.00000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"use_mtx\", subset(demographic, class == 2))\n\n\n   N    Y <NA> \n 185    6    0 \n\n         N          Y       <NA> \n0.96858639 0.03141361 0.00000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"use_mtx\", subset(demographic, class == 3))\n\n\n   N    Y <NA> \n  57    1    0 \n\n         N          Y       <NA> \n0.98275862 0.01724138 0.00000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"use_mtx\", subset(demographic, class == 4))\n\n\n   N <NA> \n  15    0 \n\n   N <NA> \n   1    0 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$use_mtx)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$use_mtx\np-value = 0.1393\nalternative hypothesis: two.sided\n\n\n\n\n\n\nCodecolnames(demographic)[colnames(demographic) == \"EEN..Y.N.\"] <- \"EEN\"\ndemographic$EEN <- na_if(demographic$EEN, \"\")\n\n\nPopulation\n\nCodedescribe_cat(\"EEN\", demographic)\n\n\n   N    Y <NA> \n 213   80   63 \n\n        N         Y      <NA> \n0.5983146 0.2247191 0.1769663 \n\n\nClass = 1\n\nCodedescribe_cat(\"EEN\", subset(demographic, class == 1))\n\n\n   N    Y <NA> \n  54   22   16 \n\n        N         Y      <NA> \n0.5869565 0.2391304 0.1739130 \n\n\nClass = 2\n\nCodedescribe_cat(\"EEN\", subset(demographic, class == 2))\n\n\n   N    Y <NA> \n 115   39   37 \n\n        N         Y      <NA> \n0.6020942 0.2041885 0.1937173 \n\n\nClass = 3\n\nCodedescribe_cat(\"EEN\", subset(demographic, class == 3))\n\n\n   N    Y <NA> \n  35   14    9 \n\n        N         Y      <NA> \n0.6034483 0.2413793 0.1551724 \n\n\nClass = 4\n\nCodedescribe_cat(\"EEN\", subset(demographic, class == 4))\n\n\n   N    Y <NA> \n   9    5    1 \n\n         N          Y       <NA> \n0.60000000 0.33333333 0.06666667 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$EEN)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$EEN\np-value = 0.7785\nalternative hypothesis: two.sided\n\n\n\n\n\n\nCodecolnames(demographic)[colnames(demographic) == \"BIOLOGIC.MONO...COMBO.WITHIN.3M\"] <- \"use_bio3\"\n\n\nPopulation\n\nCodedescribe_cat(\"use_bio3\", demographic)\n\n\n   0    1 <NA> \n 312   44    0 \n\n        0         1      <NA> \n0.8764045 0.1235955 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"use_bio3\", subset(demographic, class == 1))\n\n\n   0    1 <NA> \n  69   23    0 \n\n   0    1 <NA> \n0.75 0.25 0.00 \n\n\nClass = 2\n\nCodedescribe_cat(\"use_bio3\", subset(demographic, class == 2))\n\n\n   0    1 <NA> \n 177   14    0 \n\n         0          1       <NA> \n0.92670157 0.07329843 0.00000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"use_bio3\", subset(demographic, class == 3))\n\n\n   0    1 <NA> \n  53    5    0 \n\n        0         1      <NA> \n0.9137931 0.0862069 0.0000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"use_bio3\", subset(demographic, class == 4))\n\n\n   0    1 <NA> \n  13    2    0 \n\n        0         1      <NA> \n0.8666667 0.1333333 0.0000000 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$use_bio3)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$use_bio3\np-value = 0.0005305\nalternative hypothesis: two.sided\n\n\n\n\n\n\nCodecolnames(demographic)[colnames(demographic) == \"BIOLOGIC.WITHIN.1st.YEAR..Y.N.\"] <- \"use_bio\"\n\n\nPopulation\n\nCodedescribe_cat(\"use_bio\", demographic)\n\n\n   N    Y <NA> \n 262   94    0 \n\n        N         Y      <NA> \n0.7359551 0.2640449 0.0000000 \n\n\nClass = 1\n\nCodedescribe_cat(\"use_bio\", subset(demographic, class == 1))\n\n\n   N    Y <NA> \n  50   42    0 \n\n        N         Y      <NA> \n0.5434783 0.4565217 0.0000000 \n\n\nClass = 2\n\nCodedescribe_cat(\"use_bio\", subset(demographic, class == 2))\n\n\n   N    Y <NA> \n 156   35    0 \n\n        N         Y      <NA> \n0.8167539 0.1832461 0.0000000 \n\n\nClass = 3\n\nCodedescribe_cat(\"use_bio\", subset(demographic, class == 3))\n\n\n   N    Y <NA> \n  46   12    0 \n\n        N         Y      <NA> \n0.7931034 0.2068966 0.0000000 \n\n\nClass = 4\n\nCodedescribe_cat(\"use_bio\", subset(demographic, class == 4))\n\n\n   N    Y <NA> \n  10    5    0 \n\n        N         Y      <NA> \n0.6666667 0.3333333 0.0000000 \n\n\nFisher’s exact test\n\nCodefisher.test(demographic$class, demographic$use_bio)\n\n\n    Fisher's Exact Test for Count Data\n\ndata:  demographic$class and demographic$use_bio\np-value = 1.823e-05\nalternative hypothesis: two.sided\n\n\n\n\n\nHere, we consider the number of days from the start of the study period (2005-01-1) to when a subject was diagnosed to determine if when a subject was diagnosed has a statistically significant impact on which class a subject was assigned to.\n\nCodedemographic <- datefixR::fix_dates(demographic, \"DATE.OF.DIAGNOSIS\")\ndemographic$Days.From.Start <- as.numeric(demographic$DATE.OF.DIAGNOSIS) -\n  as.numeric(as.Date(\"2005-01-01\"))\n\ndescribe_cont(\"Days.From.Start\", demographic)\n\n         Variable   n Median   Q1   Q3 Min  Max\n1 Days.From.Start 356   2980 2280 3840   0 4710\n\n\nClass = 1\n\nCodedescribe_cont(\"Days.From.Start\", subset(demographic, class == 1))\n\n         Variable  n Median   Q1   Q3 Min  Max\n1 Days.From.Start 92   3060 2480 4000 150 4700\n\n\nClass = 2\n\nCodedescribe_cont(\"Days.From.Start\", subset(demographic, class == 2))\n\n         Variable   n Median   Q1   Q3 Min  Max\n1 Days.From.Start 191   3050 2110 3850   0 4710\n\n\nClass = 3\n\nCodedescribe_cont(\"Days.From.Start\", subset(demographic, class == 3))\n\n         Variable  n Median   Q1   Q3 Min  Max\n1 Days.From.Start 58   2840 2260 3790 424 4410\n\n\nClass = 4\n\nCodedescribe_cont(\"Days.From.Start\", subset(demographic, class == 4))\n\n         Variable  n Median   Q1   Q3  Min  Max\n1 Days.From.Start 15   2820 2360 3430 1370 4300\n\n\nANOVA\n\nCodesummary(aov(Days.From.Start ~ class, data = demographic))\n\n             Df    Sum Sq Mean Sq F value Pr(>F)\nclass         1   2713998 2713998   2.419  0.121\nResiduals   354 397129240 1121834"
  },
  {
    "objectID": "associations.html#predictive-models",
    "href": "associations.html#predictive-models",
    "title": "Association Testing",
    "section": "Predictive models",
    "text": "Predictive models\nHere, we consider whether we can accurately predict class membership using age, sex, and the variables we found to be significantly associated with class membership: smoking at diagnosis, upper gastrointestinal inflammation (Montreal L4), and being prescribed a biologic therapeutic within one year of diagnosis. We consider two types of models: multinomial logistic regression models and a random forest classifiers using the nnet and ranger packages. We use the tidymodels framework for fitting these models.\nThe data are split into a 75:25 train:test split and k-fold cross validation is used. Reported metrics and the confusion matrix are based on the test data split.\nAll classes\nFirstly, we attempt to predict class membership for all four classes.\n\nCodedemographic$class <- as.factor(demographic$class)\ndemographic$class = relevel(demographic$class, ref = 2) # \n\ndata_split <- initial_split(demographic, prop = 0.75, strata = class)\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\nfolds <- vfold_cv(train_data, v = 4)\n\nclass_rec <- recipe(class ~ age +\n                      SEX  +\n                      smoking +\n                      LOCATION +\n                      L4 +\n                      behaviour +\n                      peri +\n                      DiagFC,\n                    data = test_data)\n\n\nMultinomial logistic regression\n\nCodemlr_mod <- multinom_reg(penalty = tune(),\n                        mode = \"classification\") %>%\n  set_engine(\"nnet\")\n\nclass_wflow <- \n  workflow() %>% \n  add_model(mlr_mod) %>% \n  add_recipe(class_rec)\n\nmlr_grid <- grid_regular(\n                         penalty(),\n                         levels = 5)\n\nclass_fit <- class_wflow %>% \n    tune_grid(\n    resamples = folds,\n    grid = mlr_grid\n    )\n\nbest_mlr <- class_fit %>%\n  select_best(\"accuracy\")\n\nfinal_wf <- \n  class_wflow %>% \n  finalize_workflow(best_mlr)\n\n\nfinal_fit <- \n  final_wf %>%\n  last_fit(data_split) \n\ntemp <- final_fit %>% extract_fit_engine()\n\ntemp$call <- quote(nnet::multinom(formula = class ~ age +\n                                      SEX  +\n                                      smoking +\n                                      LOCATION +\n                                      L4 +\n                                      behaviour +\n                                      peri +\n                                      DiagFC,\n                                  data = train_data,\n                                  decay = ~0.00316227766016838, \n                                  trace = FALSE))\n\nhmm <- summary(temp)\n\n\nOdd ratios\nPoint estimates:\n\nCodeknitr::kable(round(exp(hmm$coefficients), 2)) \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nage\nSEXM\nsmokingyes\nLOCATIONL2\nLOCATIONL3\nL4yes\nbehaviour2\nbehaviour3\nperiyes\nDiagFC\n\n\n\n1\n1.35\n0.98\n0.83\n1.09\n2.51\n2.98\n0.16\n1.85\n32.65\n1.72\n1\n\n\n3\n0.47\n1.00\n0.53\n0.16\n2.16\n1.45\n1.42\n0.97\n0.80\n0.78\n1\n\n\n4\n0.06\n1.02\n1.17\n0.22\n0.34\n0.99\n1.30\n1.16\n0.86\n2.67\n1\n\n\n\n\n\nLower 95% confidence interval:\n\nCodeknitr::kable(round(exp(hmm$coefficients - qnorm(0.975) * hmm$standard.errors), 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nage\nSEXM\nsmokingyes\nLOCATIONL2\nLOCATIONL3\nL4yes\nbehaviour2\nbehaviour3\nperiyes\nDiagFC\n\n\n\n1\n0.39\n0.96\n0.44\n0.51\n1.08\n1.23\n0.06\n0.62\n32.51\n0.74\n1\n\n\n3\n0.11\n0.98\n0.25\n0.04\n0.80\n0.49\n0.56\n0.19\n0.80\n0.28\n1\n\n\n4\n0.04\n1.00\n0.36\n0.04\n0.08\n0.27\n0.29\n0.17\n0.86\n0.61\n1\n\n\n\n\n\nUpper 95% confidence interval:\n\nCodeknitr::kable(round(exp(hmm$coefficients + qnorm(0.975) * hmm$standard.errors), 2))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n(Intercept)\nage\nSEXM\nsmokingyes\nLOCATIONL2\nLOCATIONL3\nL4yes\nbehaviour2\nbehaviour3\nperiyes\nDiagFC\n\n\n\n1\n4.64\n1.00\n1.57\n2.33\n5.83\n7.23\n0.43\n5.52\n32.79\n4.02\n1\n\n\n3\n1.96\n1.02\n1.12\n0.73\n5.77\n4.26\n3.62\n4.85\n0.80\n2.20\n1\n\n\n4\n0.11\n1.04\n3.82\n1.09\n1.37\n3.72\n5.92\n7.79\n0.86\n11.71\n1\n\n\n\n\n\nMetrics\n\nCodeknitr::kable(\n  final_fit %>%\n    collect_metrics(), \n  col.names = c(\"Metric\", \"Estimator\", \"Estimate\", \"Config\"),\n  align = \"cccc\") \n\n\n\nMetric\nEstimator\nEstimate\nConfig\n\n\n\naccuracy\nmulticlass\n0.5444444\nPreprocessor1_Model1\n\n\nroc_auc\nhand_till\n0.6527881\nPreprocessor1_Model1\n\n\n\n\n\nConfusion matrix\n\nCoderesults <- final_fit %>%\n  collect_predictions() \n\nknitr::kable(table(results$class, results$.pred_class), row.names = TRUE)\n\n\n\n\n2\n1\n3\n4\n\n\n\n2\n41\n7\n0\n0\n\n\n1\n16\n8\n0\n0\n\n\n3\n10\n4\n0\n0\n\n\n4\n4\n0\n0\n0\n\n\n\n\n\nRandom forest\nMetrics\n\nCoderf_mod <- rand_forest(mtry = tune(), \n                      trees = tune(),\n                      mode = \"classification\") %>%\n  set_engine(\"ranger\",\n             num.threads = parallel::detectCores(),\n             importance = \"permutation\")\n\nclass_wflow <- \n  workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(class_rec)\n\ntree_grid <- grid_regular(mtry(range(c(1,8))),\n                          trees(),\n                          levels = 5)\n\nclass_fit <- class_wflow %>% \n    tune_grid(\n    resamples = folds,\n    grid = tree_grid\n    )\n\nbest_tree <- class_fit %>%\n  select_best(\"accuracy\")\n\nfinal_wf <- \n  class_wflow %>% \n  finalize_workflow(best_tree)\n\nfinal_fit <- \n  final_wf %>%\n  last_fit(data_split) \n\nknitr::kable(\n  final_fit %>% \n    extract_fit_parsnip() %>% \n    vi()\n)\n\n\n\nVariable\nImportance\n\n\n\nL4\n0.0056340\n\n\nsmoking\n0.0032240\n\n\nLOCATION\n0.0015034\n\n\nage\n0.0013339\n\n\nDiagFC\n0.0003739\n\n\nbehaviour\n0.0001972\n\n\nSEX\n-0.0008423\n\n\nperi\n-0.0018048\n\n\n\n\nCodeknitr::kable(\n  final_fit %>%\n    collect_metrics(), \n  col.names = c(\"Metric\", \"Estimator\", \"Estimate\", \"Config\"),\n  align = \"cccc\") \n\n\n\nMetric\nEstimator\nEstimate\nConfig\n\n\n\naccuracy\nmulticlass\n0.5111111\nPreprocessor1_Model1\n\n\nroc_auc\nhand_till\n0.6602079\nPreprocessor1_Model1\n\n\n\n\n\nConfusion matrix\n\nCoderesults <- final_fit %>%\n  collect_predictions() \nknitr::kable(table(results$class, results$.pred_class), row.names = TRUE)\n\n\n\n\n2\n1\n3\n4\n\n\n\n2\n43\n5\n0\n0\n\n\n1\n21\n3\n0\n0\n\n\n3\n14\n0\n0\n0\n\n\n4\n4\n0\n0\n0\n\n\n\n\n\nModel performance for both the multinomial logistic regression model and the random forest classifier is very poor.\nClass 2\nDue to the clinical significance of class 2 (see outcomes), the clear separation in mean class profiles between class 2 and the other classes, and the poor model performance described above, we now consider binary outcome models which attempt to predict membership, or non-membership, of class 2.\n\nCodedemographic$class2 <- rep(NA, nrow(demographic))\ndemographic[demographic[,\"class\"] == 2, \"class2\"] <- 1\ndemographic[demographic[,\"class\"] != 2, \"class2\"] <- 0\ndemographic$class2 <- as.factor(demographic$class2)\n\ndata_split <- initial_split(demographic, prop = 0.75, strata = class2)\ntrain_data <- training(data_split)\ntest_data  <- testing(data_split)\n\nfolds <- vfold_cv(train_data, v = 4)\n\nclass_rec <- recipe(class2 ~ age +\n                      SEX  +\n                      smoking +\n                      LOCATION +\n                      L4 +\n                      behaviour +\n                      peri +\n                      FC.VALUE.AT.DIAGNOSIS,\n                    data = test_data)\n\n\nLogistic regression\nAs we now considering a binary outcome, logistic regression via the glm() function is used instead of multinomial logistic regression.\nMetrics\n\nCodelr_mod <- logistic_reg(penalty = tune(),\n                        mode = \"classification\") %>%\n  set_engine(\"glm\")\n\nclass_wflow <- \n  workflow() %>% \n  add_model(lr_mod) %>% \n  add_recipe(class_rec)\n\nlr_grid <- grid_regular(penalty(),\n                        levels = 5)\n\nclass_fit <- class_wflow %>% \n    tune_grid(\n    resamples = folds,\n    grid = lr_grid\n    )\n\nWarning: No tuning parameters have been detected, performance will be evaluated\nusing the resamples with no tuning. Did you want to [tune()] parameters?\n\nCodebest_lr <- class_fit %>%\n  select_best(\"accuracy\")\n\nfinal_wf <- \n  class_wflow %>% \n  finalize_workflow(best_lr)\n\n\nfinal_fit <- \n  final_wf %>%\n  last_fit(data_split) \n\nknitr::kable(\n  final_fit %>%\n    collect_metrics(), \n  col.names = c(\"Metric\", \"Estimator\", \"Estimate\", \"Config\"),\n  align = \"cccc\") \n\n\n\nMetric\nEstimator\nEstimate\nConfig\n\n\n\naccuracy\nbinary\n0.5666667\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.6066468\nPreprocessor1_Model1\n\n\n\n\n\nConfusion matrix\n\nCoderesults <- final_fit %>%\n  collect_predictions() \n\nknitr::kable(table(results$class2, results$.pred_class), row.names = TRUE)\n\n\n\n\n0\n1\n\n\n\n0\n19\n23\n\n\n1\n16\n32\n\n\n\n\n\nRandom forest\nMetrics\n\nCoderf_mod <- rand_forest(mtry = tune(), \n                      trees = tune(),\n                      mode = \"classification\") %>%\n  set_engine(\"ranger\")\n\nclass_wflow <- \n  workflow() %>% \n  add_model(rf_mod) %>% \n  add_recipe(class_rec)\n\ntree_grid <- grid_regular(mtry(range(c(1,8))),\n                          trees(),\n                          levels = 5)\n\nclass_fit <- class_wflow %>% \n    tune_grid(\n    resamples = folds,\n    grid = tree_grid\n    )\n\nbest_tree <- class_fit %>%\n  select_best(\"accuracy\")\n\nfinal_wf <- \n  class_wflow %>% \n  finalize_workflow(best_tree)\n\n\nfinal_fit <- \n  final_wf %>%\n  last_fit(data_split) \n\nknitr::kable(\n  final_fit %>%\n    collect_metrics(), \n  col.names = c(\"Metric\", \"Estimator\", \"Estimate\", \"Config\"),\n  align = \"cccc\")\n\n\n\nMetric\nEstimator\nEstimate\nConfig\n\n\n\naccuracy\nbinary\n0.4444444\nPreprocessor1_Model1\n\n\nroc_auc\nbinary\n0.4650298\nPreprocessor1_Model1\n\n\n\n\n\nConfusion matrix\n\nCoderesults <- final_fit %>%\n  collect_predictions() \n\nknitr::kable(table(results$class2, results$.pred_class), row.names = TRUE)\n\n\n\n\n0\n1\n\n\n\n0\n17\n25\n\n\n1\n25\n23\n\n\n\n\n\nHowever, model performance remains very poor."
  },
  {
    "objectID": "associations.html#session-information",
    "href": "associations.html#session-information",
    "title": "Association Testing",
    "section": "Session information",
    "text": "Session information\n\n\nR version 4.2.0 (2022-04-22)\nPlatform: x86_64-apple-darwin17.0 (64-bit)\nlocale: en_GB.UTF-8||en_GB.UTF-8||en_GB.UTF-8||C||en_GB.UTF-8||en_GB.UTF-8\nattached base packages: stats, graphics, grDevices, datasets, utils, methods and base\nother attached packages: plyr(v.1.8.7), vip(v.0.3.2), patchwork(v.1.1.1), survminer(v.0.4.9), ggpubr(v.0.4.0), survival(v.3.3-1), ranger(v.0.13.1), yardstick(v.0.0.9), workflowsets(v.0.2.1), workflows(v.0.2.6), tune(v.0.2.0), tidyr(v.1.2.0), tibble(v.3.1.7), rsample(v.0.1.1), recipes(v.0.2.0), purrr(v.0.3.4), parsnip(v.0.2.1), modeldata(v.0.1.1), infer(v.1.0.0), ggplot2(v.3.3.6), dplyr(v.1.0.9), dials(v.0.1.1), scales(v.1.2.0), broom(v.0.8.0), tidymodels(v.0.2.0) and nnet(v.7.3-17)\nloaded via a namespace (and not attached): colorspace(v.2.0-3), ggsignif(v.0.6.3), ellipsis(v.0.3.2), class(v.7.3-20), rstudioapi(v.0.13), listenv(v.0.8.0), furrr(v.0.3.0), farver(v.2.1.0), prodlim(v.2019.11.13), fansi(v.1.0.3), lubridate(v.1.8.0), codetools(v.0.2-18), splines(v.4.2.0), knitr(v.1.39), jsonlite(v.1.8.0), pROC(v.1.18.0), km.ci(v.0.5-6), compiler(v.4.2.0), backports(v.1.4.1), assertthat(v.0.2.1), Matrix(v.1.4-1), fastmap(v.1.1.0), cli(v.3.3.0), htmltools(v.0.5.2), tools(v.4.2.0), gtable(v.0.3.0), glue(v.1.6.2), Rcpp(v.1.0.8.3), carData(v.3.0-5), DiceDesign(v.1.9), vctrs(v.0.4.1), iterators(v.1.0.14), timeDate(v.3043.102), gower(v.1.0.0), xfun(v.0.30), stringr(v.1.4.0), globals(v.0.14.0), lifecycle(v.1.0.1), renv(v.0.15.4), rstatix(v.0.7.0), future(v.1.25.0), MASS(v.7.3-57), zoo(v.1.8-10), ipred(v.0.9-12), ragg(v.1.2.2), parallel(v.4.2.0), yaml(v.2.3.5), gridExtra(v.2.3), pander(v.0.6.5), KMsurv(v.0.1-5), rpart(v.4.1.16), stringi(v.1.7.6), highr(v.0.9), foreach(v.1.5.2), lhs(v.1.1.5), hardhat(v.0.2.0), lava(v.1.6.10), systemfonts(v.1.0.4), rlang(v.1.0.2), pkgconfig(v.2.0.3), evaluate(v.0.15), lattice(v.0.20-45), datefixR(v.0.1.4), htmlwidgets(v.1.5.4), labeling(v.0.4.2), tidyselect(v.1.1.2), parallelly(v.1.31.1), magrittr(v.2.0.3), R6(v.2.5.1), generics(v.0.1.2), DBI(v.1.1.2), pillar(v.1.7.0), withr(v.2.5.0), abind(v.1.4-5), future.apply(v.1.9.0), crayon(v.1.5.1), car(v.3.0-13), survMisc(v.0.5.6), utf8(v.1.2.2), rmarkdown(v.2.14), grid(v.4.2.0), data.table(v.1.14.2), digest(v.0.6.29), xtable(v.1.8-4), textshaping(v.0.3.6), GPfit(v.1.0-8) and munsell(v.0.5.0)"
  },
  {
    "objectID": "associations.html#acknowledgments",
    "href": "associations.html#acknowledgments",
    "title": "Association Testing",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nThis work is funded by the Medical Research Council & University of Edinburgh via a Precision Medicine PhD studentship (MR/N013166/1, to NC-C)"
  },
  {
    "objectID": "associations.html#author-contributions",
    "href": "associations.html#author-contributions",
    "title": "Association Testing",
    "section": "Author contributions",
    "text": "Author contributions\nNC-C wrote the analysis. KM and CAV performed code review and contributed suggestions. RM contributed functions for calculating descriptive statistics. KM, RM and CAV provided feedback."
  },
  {
    "objectID": "associations.html#reuse",
    "href": "associations.html#reuse",
    "title": "Association Testing",
    "section": "Reuse",
    "text": "Reuse\nLicensed by CC BY  unless otherwise stated."
  }
]